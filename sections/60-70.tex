% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../img/}}}
\begin{document}

The years after 1956 are considered by many the "golden years" of AI research: thanks to considerable successes and a general wave of optimism, money was poured into the field, which thankfully generated more results and increased the hopes again. Although some interest was generated towards neural networks, this was completely shut down by a Minsky critique in 1969 (analogous to the Chomsky critique of "Verbal Behavior"). Psychology saw the rise of research into representations, categories and memory, as we will briefly overview in this section.

\subsubsection{Computer Science and Artificial Intelligence}
For what concern Computer Science and Artificial Intelligence, this period saw various directions, inspired by some of the previous research we've touched on in the previous sections. Most of them were focused on symbolic AI; at the same time, the "perceptron" proposal from McCulloch and Pitts saw some interest, before being shut down for more than ten years.

\vspace{4pt}
\textbf{Reasoning and the General Problem Solver.} As we mentioned, an important paradigm was introduced with the Logic Theorist: seeing reasoning as search. Newell and Simon, in 1959, developed what they hoped could become a general version of the LT, the General Problem Solver. Although the paradigm of reasoning as search was maintained, the GPS did not prune paths that were unlikely to lead to the goal, but used \textit{means-ends analysis} to limit search. When following MEA, a system chooses, given a current state, an action that reduces the difference between the current state and the goal state. By focusing on the difference between current and goal state, MEA improves on brute-forcing all possible choices. In addition, if knowledge about the relative importance of differences is available, the goal-seeking system can follow the path which decreases the difference most, further pruning the possible choices. The correspondance difference-action, also called operator, must be given as an input, and represents "a priori knowledge" of the problem. This separation between problem-specific knowledge and strategy of how to solve it is a relevant feature of the project, and an important point when compared with the following paradigm, expert systems.

\vspace{4pt}
\textbf{Symbols and successes.} Following the GPS, other symbol and knowledge-based systems led to great successes. In 1958, the same year in which he invented Lisp, McCarthy published "Programs with Common Sense"; in it, he described a hypothetical program that used general knowledge to search for solutions to problems (such as generating a plan to drive to the airport). Called the Advice Taker, it also allowed for additional knowledge (axioms) to be introduced during the course of operation. As such, it embodied the a simple principle of knowledge representation: manipulating a formal representation of the world and its workings as a mean to solving problems. For later purposes, we mention the Shakey project at Stanford, which used subgoals (like GPS) and logic to control a robot. Minsky, who moved to MIT in 1958, started supervising students who tackled limited problems that seemed to require intelligence to solve: these would become known as microworlds. Two of these were Daniel Bobrow's STUDENT (1967), which could solve high school algebra word problems and Tom Evans's ANALOGY (1968), that solved geometric analogy problems from IQ tests. The most famous microworld was the block world, a set of blocks on a tabletop (real or virtual), which had to be rearranged, one block at a time, according to instructions. The success of this microworld derived from the cooperation of many different researchers, as "Artificial Intelligence: a Modern Approach" reports:
\begin{quote}
    The blocks world was home to the vision project of David Huffman (1971), the vision and constraint-propagation work of David Waltz (1975), the learning theory of Patrick Winston (1970), the natural-language-understanding program of Terry Winograd (1972), and the planner of Scott Fahlman (1974).
\end{quote}\cite{russellArtificialIntelligenceModern2002}
As a fitting consequence for such successes, in 1976 Newell and Simon formulated the Physical Symbol System Hyposthesis \cite{newellComputerScienceEmpirical1976}. It states that "a physical symbol system has the necessary and sufficient means for general intelligent action"; which means that any system possessing intelligence must operate by manipulating symbols. This statement would later be challenged by many researchers.

\vspace{4pt}
\textbf{Genetic algorithms and perceptrons.} Between the late 1950s and early 1960s \cite{friedbergLearningMachinePart1958}, Friedberg started researching machine evolution (later called genetic algorithms), with scarce success. The basic idea was to make a series of small modifications to a program, then select the best-performing variant and repeat the process until the result was good enough. Unfortunately, due to how immature representation research and because of computing power constraints, these showed very limited success and the program was dropped.

Following the work of McCulloch and Pitts, research on neural networks picked up. Bernie Widrow researched his adalines\cite{widrowAssociativeStorageRetrieval1962}, while Rosenblatt researched perceptrons. In addition, in 1962 Block showed, with the perceptron convergence theorem, that if a pattern of connection strenghts that matches a certain input data exists, then the learning algorithm can adjust the strengths correctly\cite{russellArtificialIntelligenceModern2002}.

\subsubsection{DCS}


\end{document}
