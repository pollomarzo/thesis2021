% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../img/}}}
\begin{document}

The years after 1956 are considered by many the ``golden years" of AI research: thanks to considerable successes and a general wave of optimism, money was poured into the field, which thankfully generated more results and increased the hopes again. Although some interest was generated towards neural networks, this was completely shut down by a Minsky critique in 1969 (analogous to the Chomsky critique of ``Verbal Behavior"). Psychology saw the rise of research into representations, categories and memory, as we will briefly overview in this section.

\subsubsection{Computer Science and Artificial Intelligence}
For what concern Computer Science and Artificial Intelligence, this period saw various directions, inspired by some of the previous research we've touched on in the previous sections. Most of them were focused on symbolic AI; at the same time, the ``perceptron" proposal from McCulloch and Pitts saw some interest, before being shut down for more than ten years.

\vspace{4pt}
\textbf{Reasoning and the General Problem Solver.} As we mentioned, an important paradigm was introduced with the Logic Theorist: seeing reasoning as search. Newell and Simon, in 1959, developed what they hoped could become a general version of the LT, the General Problem Solver. Although the paradigm of reasoning as search was maintained, the GPS did not prune paths that were unlikely to lead to the goal, but used \textit{means-ends analysis} to limit search. When following MEA, a system chooses, given a current state, an action that reduces the difference between the current state and the goal state. By focusing on the difference between current and goal state, MEA improves on brute-forcing all possible choices. In addition, if knowledge about the relative importance of differences is available, the goal-seeking system can follow the path which decreases the difference most, further pruning the possible choices. The correspondance difference-action, also called operator, must be given as an input, and represents ``a priori knowledge" of the problem. This separation between problem-specific knowledge and strategy of how to solve it is a relevant feature of the project, and an important point when compared with the following paradigm, expert systems.

\vspace{4pt}
\textbf{Symbols and successes.} Following the GPS, other symbol and knowledge-based systems led to great successes. In 1958, the same year in which he invented Lisp, McCarthy published ``Programs with Common Sense"\cite{mccarthyProgramsCommonSense1960}; in it, he described a hypothetical program that used general knowledge to search for solutions to problems (such as generating a plan to drive to the airport). To be called the Advice Taker, it also allowed for additional knowledge (axioms) to be introduced during the course of operation. As such, it embodied an important principle of knowledge representation: manipulating a formal representation of the world and its workings as a mean to solving problems. For later purposes, we mention the Shakey project at Stanford, which used subgoals (like GPS) and logic to control a robot. Minsky, who moved to MIT in 1958, started supervising students who tackled limited problems that seemed to require intelligence to solve: these would become known as microworlds. Two of these were Daniel Bobrow's STUDENT (1967), which could solve high school algebra word problems and Tom Evans's ANALOGY (1968), that solved geometric analogy problems from IQ tests. Research based on microworld continued throughout the 1970s.

Finally, we mention the different perspective taken by Joseph Weizenbaum, then MIT professor: between 1962-1964 he created ELIZA, a natural language processing program that mimicked conversational ability while following a simple script, the most popular of which was the ``Rogerian" DOCTOR. Although the creation of it was meant to show how superficial interaction between machines and people really is, it gave (although briefly) the impression of an intelligent interaction; it did not even have any storage, so links between sentences were impossible. Its relevancy is now both historical, as it was the first attempt at creating the illusion of intelligence through human-machine interaction, and ethical, as its creation led to ethical questions regarding its usage as a therapeutic tool.

\vspace{4pt}
\textbf{Genetic algorithms and perceptrons.} Between the late 1950s and early 1960s \cite{friedbergLearningMachinePart1958}, Friedberg started researching machine evolution (later called genetic algorithms), with scarce success. The basic idea was to make a series of small modifications to a program, then select the best-performing variant and repeat the process until the result was good enough. Unfortunately, due to how immature representation research and because of computing power constraints, these showed very limited success and the program was dropped.

Following the work of McCulloch and Pitts, research on neural networks picked up. Bernie Widrow researched his adalines\cite{widrowAssociativeStorageRetrieval1962}, while Rosenblatt researched perceptrons. In addition, in 1962 Block showed, with the perceptron convergence theorem, that if a pattern of connection strenghts that matches a certain input data exists, then the learning algorithm can always adjust the strengths correctly\cite{russellArtificialIntelligenceModern2002}.

\subsubsection{DCS}
DCS research in this period was mainly focused on modeling higher-brain function, such as memory, within the new framework of cognitive psychology. At the same time, behaviorism shifted from the strict theoretical research and found itself evolving into Applied Behavior Analysis as a scientific discipline used in therapy.

\vspace{4pt}
\textbf{Behaviorism shifts closer to its current form.} In a study from 1959, ``The psychiatric nurse as a behavioral engineer"\cite{ayllonPsychiatricNurseBehavioral1959}, the authors demonstrated the effectiveness of using a token economy to reinforce adaptive behavior for patients with schizophrenia and intellectual disability. The practical application of behavioral research grew throughout the years: a journal, the ``Journal of Applied Behavior Analysis" was founded in 1968; the ``Behavior Analysis" subdivision in the American Psychological Association was introduced in 1964; the ``Applied Animal Behaviour Science" was founded in 1975.

\vspace{4pt}
\textbf{Memory research.} A series of studies on memory by different researchers helped clear the picture: Sperling focused, throught the 1960s, on sensory memory, starting with his PhD thesis at Harvard in 1959, and papers like ``The information available in brief visual presentations"\cite{sperlingInformationAvailableBrief1960} and ``Successive approximations to a model for short-term memory" (1967). Peterson worked on short term memory, with ``Short-term retention of individual verbal items."\cite{petersonShorttermRetentionIndividual1959}, and Waugh studied the difference between short-term memory and long-term memory in ``Primary memory."\cite{waughPrimaryMemory1965}. This allowed Atkinson and Shiffrin to propose, in 1968, the Atkinson-Shiffrin memory model: it viewed memory as a tripartite system, split between sensory memory, short term memory and long term memory. Although the idea of tripartite systems wasn't novel \cite{jamesPrinciplesPsychology1890} and some of the concepts it included, like rehearsal as the transfer mechanism, have been criticized by later research, it sparked additional interest in the area of memory.

\vspace{4pt}
\textbf{Milestones and Neuroscience.} In addition, 1960 was the inauguration year for Miller-Bruner center for Cognitive Studies. At the same time, neuroscience was developing: in 1962, the FitzHugh-Nagumo model was presented, as a semplification of the previous Hodgkinâ€“Huxley model. These models gave a formal background to the activation behavior of neurons (once the stimulus reached a threshold, the system is briefly excited before going back to resting state). In the same period, Katz modeled neurotrasmission across synapses \cite{katzIonicRequirementsSynaptic1967}\cite{katzReleaseNeuralTransmitter1969}, and from 1966 Kandel and others started examining biochemical reactions to learning and memory in Aplysia (a genus).

Lastly, we mention a line of research by Shepard and his student Cooper and Metzler, in which they showed that reaction time in subjects asked to determine whether a transformation was a rotation or a reflection increased linearly with rotation degree: this suggested an internal image that was being rotated.

\vspace{4pt}
Throughout the 1960s, cognitive research started to appear, and was soon to be institutionalized. Artificial Intelligence programs focused mainly on symbolic systems, but research on neural networks increased, with some important theoretical findings supporting the hopes of the ideators. This was to come to a screeching halt, with, once again, a critical (and later partly controversial) review of the literature by Minsky and funding issues.


\end{document}
