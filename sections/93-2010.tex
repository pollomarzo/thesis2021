% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../img/}}}
\begin{document}

All the paradigms we introduced in the last chapter were developed throughout this period as well, so we won't get into them again here. Still, this newfound period of success had two important features: the rekindled interest in the general problem of AI, and the availability of very large data sets. On the DCS side, the 90s brought fundamental new technologies, extensions of previous research and the use of genetic techniques.

\subsubsection{Artificial Intelligence}
As we mentioned, the (academic, first) success of the new paradigms pushed researchers to solving the``whole agent'' problem again. In particular, the new context in which agents had to learn to operate was the Internet: AI algorithms started to act as the foundations behind, for example, search engines and recommender systems. Clearly, the process of merging previous results in separate tasks had its own share of issues, but the ideas we mentioned in the previous section allowed for a more complete picture: sensory systems (whether that was speech recognition or vision) were known to provide imperfect information, so planning systems had to handle them accordingly, using probabilistic approaches. Examples of this are the two challenges set by DARPA for autonomous driving, respectively 135 miles along an unknown desert trail, completed in 2005 by STANLEY, and 22 miles in an urban environment, completed in 2007 by BOSS. This approach is known as the ``Intelligent Agents'' approach; researchers hoped that a complete agent architecture (like Newell's SOAR, recently described\cite{pressSoarCognitiveArchitecture2012}) would give researchers the tools to build intelligent systems from the interaction of agents.

\vspace{4pt}
\textbf{Big Data.} Still, the largest impact on the world of research was probably the new availability of very large datasets, thanks in no small part to the pervasive effects of the Internet. Researchers Banko and Brill\cite{bankoScalingVeryVery2001} argued that the increase in the size of the dataset (two or three-fold) would outgrow any advantage that was to be found by tweaking the algorithm. This sentiment, which is not by any mean a formal proof, is echoed throughout the machine learning industry: another article by Norvig et al mentions, in the context of learning from text,\enquote{But invariably, simple models and a lot of data trump more elaborate models based on less data}\cite{halevyUnreasonableEffectivenessData2009}.

\vspace{4pt}
Throughout the 2010s, AI research racked up a series of wins; apart from the ones already mentioned, we also note: autonomous planning and scheduling in space exploration, by REMOTE AGENT(2000, generated plans and monitored their execution), MAPGEN (2004, the previous one's successor, planned NASA's Mars Exploration Rover), MEXAR2 (2007, mission planning fo the European Space Agency's 2008 Mars mission); game playing, by DEEP BLUE in chess and Watson in Jeopardy!; logistics planning, by DART (DARPA's logistics planner for the 1991 Persian Gulf crisis). We note that DARPA mentioned the deployment of DART more than paid back their 30-year investment in AI.

\subsubsection{DCS}


the fMRI, the TMS, the current version of the PET scan

\end{document}
