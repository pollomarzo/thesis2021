% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../img/}}}
\begin{document}

During this period, AI left the connectionism world behind, partly following a Minsky literature review in 1969. Meanwhile, symbolic research continued, and found its new paradigm: using domain-specific knowledge to solve bigger, more complicated tasks in \textit{specific} fields. The DCS domain expanded on their previous views, and some important points of contact with AI were explored.

\subsubsection{Computer Science}
As we mentioned, research in this period mainly focused on symbolic systems. The addition of context and domain specific knowledge shifted interest from imitating human or semi-human intelligence to strictly solving problems as well as possible. Although these \textit{expert systems} worked well, a series of companies and projects that overpromised advances were born, which lead to what is called "AI winter", when companies and nations realized their hope was, sometimes, unfounded.

\vspace{4pt}
\textbf{Minksy and perceptrons.}
As we mentioned in the last section, research on connectionist models, although less popular than symbolic models, kept going. One of the most relevant techniques for training neural networks was introduced in 1969\cite{brysonAppliedOptimalControl1969}, although the original research was about optimal control instead of machine learning. This is considered the first description of modern back-propagation\cite{lecunTheoreticalFrameworkBackpropagation1988}, but their version was never applied to machine learning, where backpropagation would be rediscovered in the 1980s.
Instead, research on perceptrons slowed down considerably after 1969. In the words of the standard textbook on the topic \enquote{The subsequent demise of early perceptron research efforts was hastened (or, the authors later claimed, merely explained) by the book Perceptrons, which lamented the field's lack of mathematical rigor ... and noted the lack of effective learning algorithms for multilayer networks}. The book in question, Perceptrons\cite{minskyPerceptronIntroductionComputational1969}, also noted other theoretical limitations of perceptrons, some of which may have been misinterpreted and so contributed to the general feeling towards perceptrons in general. In the last chapter the authors mention (shortsightedly, in hindsight) that multilayer neural nets would be a ``sterile" extension.

\vspace{4pt}
\textbf{Integrating knowledge.}\sidekeyword{Expert systems}
The approach so far (consider for example the efforts behind the GPS) had been to solve problems by modeling human thinking processes in their most general form, to be then applied to a specific problem. With DENDRAL, a research project that started in 1965, the creators explored a different approach: it used a large number of special-purpose rules, extracted from analytical chemists, to infer molecular structure of molecules from mass spectrometer data and the elementary formula of the molecule. As such, it became the first knowledge-intensive system.
It was soon to be followed by other expert systems, such as MYCIN, which diagnosed blood infections, and gave rise to a new, very successful paradigm. Its successes brought AI into the commercially viable technologies, and the increase of demand brought an increased interest in knowledge representation schemes: in particular, the two main approaches were logic-based and frame-based. Frames were a Minsky proposal\cite{minskyFrameworkRepresentingKnowledge1975} to organize facts about objects and events into a large hierarchical taxonomy.

\vspace{4pt}
\textbf{Natural language understanding.}\sidekeyword{Microworlds}
In the previous section, we introduced research on microworlds.
The most famous microworld was the block world, a set of blocks on a tabletop (real or virtual), which had to be rearranged, one block at a time, according to instructions. The success of this microworld (SHRDLU was the name of the successful program \cite{winogradProceduresRepresentationData1971}) derived from the cooperation of many different researchers, as ``Artificial Intelligence: a Modern Approach" reports:

\begin{quote}
    The blocks world was home to the vision project of David Huffman (1971), the vision and constraint-propagation work of David Waltz (1975), the learning theory of Patrick Winston (1970), the natural-language-understanding program of Terry Winograd (1972), and the planner of Scott Fahlman (1974).
\end{quote}\cite{russellArtificialIntelligenceModern2002}
As a fitting consequence for such successes, in 1976 Newell and Simon formulated the Physical Symbol System Hyposthesis \cite{newellComputerScienceEmpirical1976}. It states that "a physical symbol system has the necessary and sufficient means for general intelligent action"; which means that any system possessing intelligence must operate by manipulating symbols. This statement would later be challenged by many researchers.

Natural language understanding was another area in which domain knowledge had vast effects: the success of the block world-natural language program was in fact due to its specificity, and a series of programs followed it\cite{schankScriptsPlansGoals1977}\cite{wilenskyUnderstandingGoalbasedStories1978}\cite{schankComputerUnderstanding1981}, which all focused on understanding natural language by reasoning with the knowledge required.

\subsubsection{DCS}
During the 1970s, Cognitive Science went from a collection of studies with similar intentions to a true discipline, complete with relevant courses, journal and grants for research. Points of contact with existing AI research became more common, as did researchers working in both fields. Some philosophers, possibly accusing the weight of the wrongful predictions of AI research, raised critiques towards the field in general, while others began developing collective theories of mind, from Fodor's functionalism to the Computational Theory of Mind.

\vspace{4pt}
\textbf{Points of contact.}\sidekeyword{ELIZA and therapy}
In this paragraph, we will highlight two relevant contact points during this period. The first is the result of a collaboration between Gordon Bower and one of his students, John Anderson. Bower had gone from learning theory and animal testing, to mathematical models of learning to, finally, cognitively oriented work about mental representation, such as the study of chunking for short-term memory usage\cite{GordonBowerPsycNET}. Their cooperation would give rise to a semantic network model named HAM, later described in their 1973 boook Human Associative Memory\cite{andersonHumanAssociativeMemory1973}. Anderson would keep working on it, later adding a production system, increasing the types of nodes and links between nodes, and explaining the time it takes to perform a task as due the matching for the production system, until he would publish the ACT-R architecture, still in research today.

The second important connection was a clash between the author of ELIZA, mentioned in the last section, and the psychiatrist Kenneth Colby. Colby expanded on the work of Weizenbaum\cite{colbyComputerMethodPsychotherapy1966}, and wrote what he considered was a ``computer program which can conduct psychotherapeutic dialogue", with which Weizenbaum clearly disagreed. Later, in 1976, Weizenbaum published ``Computer Power and Human Reason"\cite{weizenbaumComputerPowerHuman1976}, where he declares that computers should never be allowed to make important decisions as they would always lack compassion and wisdom.

\vspace{4pt}
\textbf{Putnam, Fodor, and the Computational Theory of Mind.}\sidekeyword{Mind as information processing system}
In this section, our aim is to give a brief introduction to the philosophical path around the Computational Theory of Mind (CTM), a family of theories and views which hold that the human mind is an information processing system, and as such cognition and consciousness (sometimes not both, according to the specific theory) are a form of computation. Although it was introduced in 1961 by Putnam \cite{horstComputationalTheoryMind2003}, it was developed by Fodor throughout the following decades. The Stanford Encyclopedia defines the CTM as combining ``an account for reasoning with an account of the mental states''. Of these, the second one (Representational Theory of Mind), argues that intentional mental states, such as beliefs, are relations between ``a thinker and symbolic representations of the content of the states''(`I believe there is a book on the table' would be the functional \textit{belief} relation between me and the mental, symbolic representation of `there is a book on the table'). The first, instead, maintains that reasoning involves the symbolic representations \textit{only} in their non-semantic, syntactic properties. As such, this process can be considered a formal symbol manipulation, which qualifies as computation.

The relational character of mental states was initially introduced by Fodor in 1978\cite{fodorPropositionalAttitudes1978}, where he identified mental states as a three-way relationship between the individual, representations and propositional contents.

\vspace{5pt}
We believe that, seen in this context, the Computational Theory of Mind is a coherent theory for a few different viewpoints in the history of AI. At the same time, as we have seen in this section, AI research steered away from such open-ended questions in favor of technical and engineering achievemnts, based on restricted domains, background knowledge and specific rules: a far cry from the cognitive model the CTM would imply, and detached from the connectionist possibilities it considered just a few years prior.

\end{document}
