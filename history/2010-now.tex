% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\begin{document}

Entering the 2010, machine learning and non-symbolic approaches had begun to capture the market, both in research and in commercial applications. In the previous section, we stressed how the absolute most relevant, impactful and impressive advance was in fact the \textit{availability} of large-scale datasets. Here, we will see what this caused, and what the current state-of-the-art is able to do. We will also use this opportunity to consider what the level of expertise required is, and briefly describe the two main frameworks for creating and training deep learning networks.

Before reading on, if you do not have experience with basic machine learning and neural networks, we ask you to read Appendix A. This content was moved to an appendix not because of its secondary importance, but only to avoid making this chapter too long. In what follows, Appendix content won't be re-introduced.

\subsection{AI}
Throughout the recent years, the AI field has boomed, in no small part thanks to deep learning's success. Wikipedia points to the ``big bang" of deep learning as early as 2009, when researchers started training deep learning neural networks on Nvidia GPUs. Others\cite{parloffWhyDeepLearning2016} point to the ImageNet victory in late 2012, or a related paper a couple months prior\cite{ciresanMulticolumnDeepNeural2012}, but by now deep learning has become one of the areas of Computer Science with the highest research output. We will now consider every field in which it has obtained significant advantages, with some specific architectures explained.

\vspace{4pt}
\textbf{Computer Vision.} Research in Computer Vision is varied, so a complete description is impossible for us. A few of the categories are:
\begin{itemize}
    \item \textit{Image Classification.} It entails assigning a label to an image. The standard architecture has been a Residual Neural Network, which is a convolutional neural network in which some layers are skipped, resembling a structure seen in the brain. Recent works involves making ResNets more efficient, and with less parameters for equivalent state-of-the-art performance (about 86.5\%)\cite{tanEfficientNetV2SmallerModels2021}\cite{brockHighPerformanceLargeScaleImage2021}.
    \item \textit{Image Segmentation.} It consists of partitioning an image into sets of pixels with a label assigned. Recent efforts include using the encoder of an EfficientNet and the decoder of a UNet (a 2015 architecture \cite{ronnebergerUNetConvolutionalNetworks2015}), and running the CNN obtained on unstructured data \cite{bahetiEffUNetNovelArchitecture2020}.
    \item \textit{Object Detection.} The name is fairly self-explanatory. Recent work includes experiments with new bounding box shapes and loss functions \cite{zhangVarifocalNetIoUawareDense2020}.
\end{itemize}

\vspace{4pt}
\textbf{Natural Language Processing.} The most important recent NLP model is GPT-3. \textit{It is a model that is trained via the Generalized Progressive Transformer (GPT) framework. GPT is a transformation-based neural network that has the advantage of requiring fewer parameters than ResNets. GPT-3 is a model that is trained with TensorFlow. The resulting model is significantly more efficient than ResNets (around 70\% of the parameters), but it is not as efficient as ResNets when making discrete predictions.} In fact, the italics section was generated by GPT-3, after giving ``What is Image Classification" and the previous ``Image Classification'' description as prompts, and asking ``What is GPT-3?''. The reader may now be able to more easily understand why the paper in which it was presented contained a section warning of the model's potential dangers\cite{brownLanguageModelsAre2020}. Its full version has 175 billion parameters. \todo{generate the introduction as well. then they'll see what is up}




\end{document}
