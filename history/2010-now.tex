% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\begin{document}

Entering the 2010, machine learning and non-symbolic approaches had begun to capture the market, both in research and in commercial applications. In the previous section, we stressed how the absolute most relevant, impactful and impressive advance was in fact the \textit{availability} of large-scale datasets. Here, we will see what this caused, and what the current state-of-the-art is able to do. We will also use this opportunity to consider what the level of expertise required is, and briefly describe the two main frameworks for creating and training deep learning networks.

\subsection{AI}
Throughout the recent years, the AI field has boomed. Wikipedia points to the ``big bang" of deep learning as early as 2009, when researchers started training deep learning neural networks on Nvidia GPUs.




\subsection{Current State of Affairs}

\end{document}
