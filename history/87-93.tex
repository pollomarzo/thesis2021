% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../img/}}}
\begin{document}

Unfortunately, the enthusiasm for expert systems did not last long. As it had happened before, the hype behind AI research was too great for its own good, and towards the end of the 80s the market suffered a few serious blows. As always, though, a new wind was blowing, with several new paradigms. One in particular was very closely related to DCS research: embodied cognition.

\subsection{Computer Science, AI and engineering}
The shortcomings of expert systems, serious as they may be, had their main effect on the economic side, as research continued. Here, we will go over the reasons behind the economic crash and the new perspective in research.

\vspace{4pt}
\textbf{Expert Systems and the Hype.}\sidekeyword{Expert systems issues}
The downfall of expert systems was foreseen by some in the research community: from a 1984 article \parencite{universityWhyComputersCan1984},
\begin{quote}
    Yet Minksy and Schank contend that today's systems are largely based on 20-year-old programming techniques that have merely become practical as computer power got cheaper. Truly significant advances in computer intelligence, they say, await future breakthoughs in programming.
\end{quote}

Although their argument was a theoretical one, the practical implications had a large impact on the market: in the late 1980s, desktop computers were slowly overtaking specialized and expensive Lisp machines. In 1987, the reasons to buy them simply ended, and a large industry fell overnight.

The expert systems themselves started to show their flaws: they couldn't be updated, could not learn, and made large mistakes when given unusual innputs. Some issues with them had been shown years earlier, like the qualification problem (the inability of listing all the necessary preconditions for an action in the real world to have its intended effect). They worked in very specific scenarios, but were not as successful a recipe as they had been presented. Some of the initiatives launched were retracted, and funding dwindled \parencite{mccorduckMachinesWhoThink2004}.

\vspace{4pt}
\textbf{Robotics.} As a direct consequence of the ``lowering the mind into the body'', robotics went back to the forefront of AI research. In 1990, Brooks published ``Elephants Don't Play Chess" \parencite{brooksElephantsDonPlay1990}, in which he argued that symbols are unnecessary for cognition, because
\begin{quote}
    the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.
\end{quote}

This is obviously against the Physical Symbol System Hypothesis, and represents a general awakening towards robotics-based approaches. In fact, symbol-sustaining researchers such as Minsky felt similarly, for what concerns focusing on lower-level processing; in 1986 Minsky writes \enquote{In general, we're least aware of what our minds do best, [...] we're more aware of simple processes that don't work well than of complex ones that work flawlessly} \parencite{minskySocietyMind1986}. The comparative difficulty of sensorimotor skills compared to reasoning is considered the Moravec's paradox:
\begin{quote}
    it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility
\end{quote} \parencite{moravecMindChildrenFuture1988}
This general feeling led to research into Behavior-Based Robotics (robots that exhibit complex behavior while having little internal modeling state) and Nouvelle AI, pioneered by Brooks himself, working on situated robots (robots interacting with their sensors and their environment) with intelligence close to one of an insect.

\vspace{4pt}
\textbf{Connectionism returns!} Around the mid-1980s, following the `downfall' of expert system and symbolic approaches in general, the connectionist approach based on \textit{neural networks} made a strong resurgence. A new algorithm for training such networks was rediscovered by at least four different groups \parencite{russellArtificialIntelligenceModern2002}: it was the same algorithm found in 1969 by Bryson and Ho. We presented this research as it is usually presented: as if in antithesis to symbolic approaches. As we will see in a following paragraph, though, research was beginning to shift to a cooperative view, using complementary approaches to explaining cognition. The field went on to bifurcate into a CS and engineering-focused strand (exploring possible neural architectures, determining their properties) and a neuroscience/empirical-focused strand (modeling biological neurons as accurately as possible).

\vspace{4pt}
\textbf{Probabilistic reasoning.}\sidekeyword{Dealing with uncertain data}
We mentioned how brittle expert systems were found to be when they were applied in the real world: this drove researchers to a more scientific approach, aiming at reproducible experiments instead of philosophical claims, building on existing theories instead of constantly introducing new approaches, and including probability instead of Boolean logic. This has been called the victory of the \textit{neats}, but it would not stand forever; the recent interest in deep learning has shown impressive results from an overall \textit{scruffies}-based philosophy (new ideas are proposed, tested and evolved quickly, without always completing the mathematical background). Probabilistic reasoning was pushed forward by a 1988 piece by Judea Pearl \parencite{pearlProbabilisticReasoningIntelligent1988}, who introduced an efficient formalism for dealing with uncertain data as well as practical algorithms.

\subsection{DCS: between philosophy, psychology and neuroscience}
As we said, the disillusion towards expert systems led some researchers to advocating for a new approach, based on the physical world and robotics. They considered abstract thinking to be the least interesting human skill, and argued for ``lowering'' the mind into the body. The approach was not new: we remind the reader about the impact of cybernetics on the birth of the field of AI.

\vspace{4pt}
\textbf{Marr and computational neuroscience.}\sidekeyword{Levels of analysis}
David Marr had a similar approach towards vision, about a decade earlier. With papers in 1969, 1970 and 1971 he proposed computational theories on cerebellum \parencite{marrTheoryCerebellarCortex1969}, neocortex \parencite{marrTheoryCerebralNeocortex1970}, and hippocampus (which he called `archicortex') \parencite{marrSimpleMemoryTheory1971}. Afterwards, he focused on vision, together with the Italian researcher Tomaso Poggio. To them, vision was to be understood `bottom-up', focusing on the physical level before any symbolic processing. They considered vision an information processing system, to be analyzed at three levels \parencite{marrUnderstandingComputationUnderstanding1976}:

\begin{itemize}
    \item \textit{Computational level.} What the system does and why.
    \item \textit{Algorithmic level.} How the system does what it does, and with which processes.
    \item \textit{Physical level.} How the system is implemented.
\end{itemize}

The system may seem very simple; nonetheless, the idea of levels of analysis and its similarity to computational approaches signal the resurgence of interest in computational neuroscience that was on the horizon.

\vspace{4pt}
\textbf{Fodor and modularism.}\sidekeyword{Encapsulated, innate modules}
The interest toward lower level systems wasn't new, especially in the philosophical side of research. Fodor had been advocating for a different notion to understand the mind, also very reminiscent of technical paradigms in Computer Science: modularism. As it was introduced in 1983 \parencite{fodorModularityMind1983} and developed in the decades since, it considered a system (i.e. the mind) modular if it was at least partially composed by subunits, innate neural structures with distinct, evolutionary-developed functions. The definition of module changed, but the initial proposal contained 9 features that characterize such systems; of these, we mention:
\begin{enumerate}
    \item \textbf{Information inaccessibility and encapsulation}. The direction of information flow is restricted; for example, althuogh you may be aware of perceptional issues while watching an optical illusion, the perception will not change.
    \item \textbf{Speed and superficiality}. Modular systems are mostly fast (Fodor considers roughly half a second) and concern superficial concept: in Fodor's book this may be interpreted as computationally simple (few calculations) or informationally simple (general); both may be true, and Fodor generally excludes the possibility of modules working with `theoretical' concepts such as ``turbine" or ``proton".
    \item \textbf{Dissociability}. A system is functionally dissociable if it can be damaged without significant impairment to other systems. This is reminiscent of studies on aphasia and other brain injuries which leave other capabilities perfectly untouched.
    \item \textbf{Innateness}.
\end{enumerate}
Fodor considers relatively low-level systems of the mind to be modular (like perception or language), while high-level systems are not to be understood by modularism. Following thinkers would go on to expand the idea to ``Massive Modularity'', arguing all elements of the mind are in fact modular.

\vspace{5pt}
As we saw in this section, the crisis of expert system, while it crippled the overly excited AI market, coincided with a renewed interest towards embodied and situated systems. This was a common thread between AI and DCS research. Still, expert system did not disappear, but coexist with other approaches: as an example, paradigms introduced with them are still the basis for modern knowledge representation techniques. We close this chapter by mentioning that a companion theory to situated robotics (closely related to Nouvelle AI) in the DCS was Situated Cognition, which slowly emerged at the end of the twentieth century. Because it argues for learning as and individual's increasingly effective performance across situations, with cognition inseparable from the context, it is closer to Skinner's behavior analysis than previous storage-and-retrieval-based theories.


\end{document}