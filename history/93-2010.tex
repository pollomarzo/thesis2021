% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../img/}}}
\begin{document}

Most of the paradigms we introduced in the last chapter were developed throughout this period as well, so we won't get into them again here. Still, this newfound period of success had two important features: the rekindled interest in the general problem of AI, and the availability of very large data sets. On the DCS side, the 90s brought fundamental new technologies, extensions of previous research and new all-encompassing frameworks.

\subsection{Artificial Intelligence}
As we mentioned, the (academic, first) success of the new paradigms pushed researchers to solving the ``whole agent'' problem again. In particular, the new context in which agents had to learn to operate was the Internet: AI algorithms started to act as the foundations behind, for example, search engines and recommender systems. Clearly, the process of merging previous results in separate tasks had its own share of issues, but the ideas we mentioned in the previous section allowed for a more complete picture: sensory systems (whether that was speech recognition or vision) were known to provide imperfect information, so planning systems had to handle them accordingly, using probabilistic approaches. Examples of this are the two challenges set by DARPA for autonomous driving, respectively 135 miles along an unknown desert trail, completed in 2005 by STANLEY, and 22 miles in an urban environment, completed in 2007 by BOSS. This approach is known as the ``Intelligent Agents'' approach; researchers hoped that a complete agent architecture (like Newell's SOAR\cite{pressSoarCognitiveArchitecture2012}) would give researchers the tools to build intelligent systems from the interaction of agents.

\vspace{4pt}
\textbf{Big Data.} Still, the largest impact on the world of research was probably the new availability of very large datasets, thanks in no small part to the pervasive effects of the Internet. Researchers Banko and Brill\cite{bankoScalingVeryVery2001} argued that the increase in the size of the dataset (two or three-fold) would outgrow any advantage that was to be found by tweaking the algorithm. This sentiment, which is not by any means a formal proof, is echoed throughout the machine learning industry: another article by Norvig et al mentions, in the context of learning from text,\enquote{But invariably, simple models and a lot of data trump more elaborate models based on less data}\cite{halevyUnreasonableEffectivenessData2009}.

\vspace{4pt}
\textbf{Multi-Layer Perceptrons and further.} This paragraph contains technical terms: for the interested reader, Appendix A is available; reading Appendix A is strongly recommended before the next section. Multi-Layer Perceptrons were not one of the main direction of research, at this point. Nonetheless, research continued: LeCun applied backpropagation to a deep (i.e. with multiple hidden layers) network to recognize handwritten ZIP codes in 1989\cite{lecunBackpropagationAppliedHandwritten1989}. From this, a previous method to recognize 3D objects (matching a handcrafted 3D object model with 2D images) was adapted by Weng in 1992\cite{wengCresceptronSelforganizingNeural1992} to learn how to combine the 2D images to recognize 3D objects (in cluttered scenes) without supervision: the features that were once hand-merged were converted to convolutional layers. This paper also introduced max-pooling. Following research includes multi-layer boolean networks\cite{decarvalhoIntegratedBooleanNeural1994}, slowly training six fully connected layers\cite{hintonWakesleepAlgorithmUnsupervised1995}, extending the feed-forward approach to include lateral and backwards connections\cite{behnkeNeuralAbstractionPyramid2003}, but both shallow and deep learning Artificial Neural Networks never outperformed Hidden Markov Models (HMM); note that these were using generative models of speech, pronunciation dictionaries and acoustic models. \todo{maybe include a brief overview of HMM. \href{https://mi.eng.cam.ac.uk/~mjfg/mjfg_NOW.pdf}{this gives a good overview!}}
In 1997, Hochreiter and Schmidhuber introduced long short-term memory cell architecture\cite{hochreiterLongShortTermMemory1997}, still in use today. Still, as a 2007 paper reports \cite{galesApplicationHiddenMarkov2007}, \enquote{almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs}.

\vspace{4pt}
Throughout the 2010s, AI research racked up a series of wins; apart from the ones already mentioned, we also note: autonomous planning and scheduling in space exploration, by REMOTE AGENT(2000, generated plans and monitored their execution), MAPGEN (2004, the previous one's successor, planned NASA's Mars Exploration Rover), MEXAR2 (2007, mission planning fo the European Space Agency's 2008 Mars mission); game playing, by DEEP BLUE in chess and Watson in Jeopardy!; logistics planning, by DART (DARPA's logistics planner for the 1991 Persian Gulf crisis). We note that DARPA mentioned the deployment of DART more than paid back their 30-year investment in AI.

\subsection{DCS}
Throughout the end of the twentieth century, important technical advances allowed the resurgence in interest towards neuroscience to spike: the tools that were developed during this period would go on to become a staple in neuroscientific research. Theoretical research, instead, now included models of cognition that explored cooperative work and genetic influence; philosophical models went in a similar direction. In this section, we will then explore the difference in approach between Grand Unified Theories and specialized, expansive theories.

\vspace{4pt}
\textbf{New technology.} There were four technologies that would go on to be instrumental in the study of the brain: fMRI, TMS, PET and NIRS. The functional Magnetic Resonance Imaging measures brain activity by monitoring blood flow. The insight of its relevance belongs to the 1890s with Angelo Mosso, and the theory behind it is based on a discovery in 1936 of the different reaction of oxygen-rich and oxygen-depleted blood with Hb (hemoglobin), but the technical usage, based on works on rodents\cite{thulbornRoleFerritinHemosiderin1990}\cite{ogawaBrainMagneticResonance1990}, was only available from 1990. The first usage on humans belongs to 1992\cite{kwongDynamicMagneticResonance1992}. The TMS, instead, can be used to both monitor and stimulate; although the first stable devices appeared around 1985, the FDA approval came in 2008\cite{horvathTranscranialMagneticStimulation2011}. Near-infrared Spectroscopy (NIRS) uses the near-infrared region for spectroscopy, and its first clinical application was seen in 1994\cite{ferrariBriefReviewHistory2012}. The PET-CT scanner, based on techniques in use since the 70s, was the first to use a cylindrical array of sensors, and was named by Time as the medical invention of the year.

\vspace{4pt}
\textbf{Radicalizing the Computational Theory of Mind.} A couple chapters ago, we briefly went over the traditional Computational Theory of Mind. The CTM as we described it attempts to keep processes of reasoning and symbols entirely inside the mind itself. Externalist views, though, point out that, for example, unknown property of objects are external to the mind and cannot be constrained in representations existing within it (to this, Fodor responded by including in the CTM a causal account for mental content: a mental representation R only stands for a real-world object X if Rs are reliably caused by Xs). A more radical externalist thesis holds that cognition is both \textit{embodied} and \textit{embedded}. Embodied, in the sense that perception, action, and even reasoning use tissues and material that goes beyond the neurons in the brain, and with it they involve non-representational, non-computational bodily skills and processes. Embedded, not only as in ``interacting with the environment driven by inputs and outputs'', but also in the sense that things outside the organism, whether that's books, prostethes, or the Internet, are a fundamental part of cognition itself. This view was put forward mainly by Clark\cite{clarkNaturalbornCyborgsMinds2003} and Chalmers\cite{clarkExtendedMind1998b}, and is the backbone behind the Extended Mind Thesis. As such, these externalist views can be considered an extension of `modest' CTM (meaning that not \textit{all} aspects of the mind have to be computational, so the `offloaded' portion isn't) or a new framework in itself.

\vspace{4pt}
\textbf{Grand Unified Theories and expansive theories.}\sidekeyword{Deriving vs explaining intelligence}
In this last paragraph, we will briefly mention two unified theories and two `expansive' projects that started in this period. By Grand Unified Theory we mean a theory that starts with some relatively simple concept, and derives the behavior of the brain without ad-hoc measures for every portion or process; normally, GUTs are symbolic, as a model of the brain that simulates brain activity by simulating real-world neurons would not be considered a GUT.
The first GUT we present is the Free Energy Principle. First introduced by Karl Friston in 2006\cite{fristonFreeEnergyPrinciple2006}, it views the mind (and systems in general) as minimizing the difference between its internal model of the world and the real-bounded perception. Because of its \textit{very} complex nature, we won't get into its formal definition here; the two important features we wish to note is that it has later been acknowledged by its creator as not falsifiable, and the relevancy of the interest towards the backward pathways, from the signal-processing areas of the cortex back to the sensory ones. According to this theory, these pathways would carry predictions, and by comparing the two directions the brain would be able to calculate its error.
The second GUT we mention is the Integrated Information Theory: introduced by Tononi in 2004\cite{tononiInformationIntegrationTheory2004}, it takes the existence of consciousness as certain, and reasons about the properties that the physical substrate needs in order to implement it. Others axioms include the compositional nature of consciousness but also its irreducibility (as in, inability to be subdivided) to its components, and how conscious experience is definite, both in content and in spatio-temporal unit.

In contrast, let us consider the specialized, not interpretability-oriented, expansive view: instead of reasoning about cosciousness or how to `force' the mind into a theoretical framework, these theories (or better, projects), their aim is to simulate it in order to then understand it. Two relevant projects we mention is the Blue Brain Project, founded in 2005 at the École Polytechnique Fédérale de Lausanne, which runs a  simulated brain made up of biologically realistic models of neurons. Interestingly, its initial goal, reached in 2006, was to create a simultaed rat neocortical column, considered by some the smallest functional unit of the neocortex and of great interest for Kriston. Another similar project is the Semantic Pointer Architecture Unified Network, a cognitive architecture pioneered at the University of Waterloo. Consisting of 2.5 million simultated neurons, its capable of recognizing numbers, memorizing them and even writing them down with a robotic arm. Its subsystems are organized to resemble relevant brain regions.

\textbf{Ethics of AI.} To be written. Just a paragraph... come on\todo{DO IT!}


\vspace{5pt}
In this section, we explored some of the new paradigms proposed by researchers for the comprehension of the mind, and put forward what we believe is the single largest impact on AI of the period: the availability of huge datasets. In the following section, we will explore where this led us, and what the state of the art is now across disciplines and problems.

\end{document}
