
@article{nunezWhatHappenedCognitive2019,
	title = {What happened to cognitive science?},
	volume = {3},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-019-0626-2},
	doi = {10.1038/s41562-019-0626-2},
	abstract = {More than a half-century ago, the ‘cognitive revolution’, with the influential tenet ‘cognition is computation’, launched the investigation of the mind through a multidisciplinary endeavour called cognitive science. Despite significant diversity of views regarding its definition and intended scope, this new science, explicitly named in the singular, was meant to have a cohesive subject matter, complementary methods and integrated theories. Multiple signs, however, suggest that over time the prospect of an integrated cohesive science has not materialized. Here we investigate the status of the field in a data-informed manner, focusing on four indicators, two bibliometric and two socio-institutional. These indicators consistently show that the devised multi-disciplinary program failed to transition to a mature inter-disciplinary coherent field. Bibliometrically, the field has been largely subsumed by (cognitive) psychology, and educationally, it exhibits a striking lack of curricular consensus, raising questions about the future of the cognitive science enterprise.},
	language = {English},
	number = {8},
	urldate = {2021-06-13},
	journal = {Nature Human Behaviour},
	author = {Núñez, Rafael and Allen, Michael and Gao, Richard and Miller Rigoli, Carson and Relaford-Doyle, Josephine and Semenuks, Arturs},
	month = aug,
	year = {2019},
	note = {tex.copyright: 2019 Springer Nature Limited},
	pages = {782--791},
	file = {Snapshot:/home/paolo/zotero_data/storage/T5773YPD/s41562-019-0626-2.html:text/html;Núñez_et_al-2019-What_happened_to_cognitive_science.pdf:/home/paolo/zotero_data/storage/DFSFK4LK/Núñez_et_al-2019-What_happened_to_cognitive_science.pdf:application/pdf},
}

@article{besoldNeuralSymbolicLearningReasoning2017,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {A} {Survey} and {Interpretation}},
	shorttitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	url = {http://arxiv.org/abs/1711.03902},
	abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
	language = {English},
	urldate = {2021-06-13},
	journal = {arXiv:1711.03902 [cs]},
	author = {Besold, Tarek R. and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
	month = nov,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Besold_et_al-2017-Neural-Symbolic_Learning_and_Reasoning.pdf:/home/paolo/zotero_data/storage/6KHLV2SR/Besold_et_al-2017-Neural-Symbolic_Learning_and_Reasoning.pdf:application/pdf},
}

@article{garcezNeurosymbolicAI3rd2020,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {Neurosymbolic {AI}},
	url = {http://arxiv.org/abs/2012.05876},
	abstract = {Current advances in Artiﬁcial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by inﬂuential thinkers. Many have identiﬁed the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	language = {English},
	urldate = {2021-06-13},
	journal = {arXiv:2012.05876 [cs]},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	file = {Garcez_Lamb-2020-Neurosymbolic_AI.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2020/Garcez_Lamb-2020-Neurosymbolic_AI.pdf:application/pdf},
}

@article{sarkerNeuroSymbolicArtificialIntelligence2021,
	title = {Neuro-{Symbolic} {Artificial} {Intelligence}: {Current} {Trends}},
	shorttitle = {Neuro-{Symbolic} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2105.05330},
	abstract = {Neuro-Symbolic Artiﬁcial Intelligence – the combination of symbolic methods with methods that are based on artiﬁcial neural networks – has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.},
	language = {English},
	urldate = {2021-06-13},
	journal = {arXiv:2105.05330 [cs]},
	author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
	month = may,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Sarker_et_al-2021-Neuro-Symbolic_Artificial_Intelligence.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2021/Sarker_et_al-2021-Neuro-Symbolic_Artificial_Intelligence.pdf:application/pdf},
}

@book{sunConnectionistSymbolicIntegrationUnified2013,
	title = {Connectionist-{Symbolic} {Integration}: {From} {Unified} to {Hybrid} {Approaches}},
	isbn = {978-1-134-80206-7},
	shorttitle = {Connectionist-{Symbolic} {Integration}},
	abstract = {A variety of ideas, approaches, and techniques exist – in terms of both architecture and learning – and this abundance seems to lead to many exciting possibilities in terms of theoretical advances and application potentials. Despite the apparent diversity, there is clearly an underlying unifying theme: architectures that bring together symbolic and connectionist models to achieve a synthesis and synergy of the two different paradigms, and the learning and knowledge acquisition methods for developing such architectures. More effort needs to be extended to exploit the possibilities and opportunities in this area. This book is the outgrowth of The IJCAI Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid Approaches, held in conjunction with the fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95). Featuring various presentations and discussions, this two-day workshop brought to light many new ideas, controversies, and syntheses which lead to the present volume. This book is concerned with the development, analysis, and application of hybrid connectionist-symbolic models in artificial intelligence and cognitive science. Drawing contributions from a large international group of experts, it describes and compares a variety of models in this area. The types of models discussed cover a wide range of the evolving spectrum of hybrid models, thus serving as a well-balanced progress report on the state of the art. As such, this volume provides an information clearinghouse for various proposed approaches and models that share the common belief that connectionist and symbolic models can be usefully combined and integrated, and such integration may lead to significant advances in understanding intelligence.},
	language = {English},
	publisher = {Psychology Press},
	author = {Sun, Ron and Alexandre, Frederic},
	month = apr,
	year = {2013},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / General},
}

@article{grayWelcomeCognitiveScience2019,
	title = {Welcome to {Cognitive} {Science}: {The} {Once} and {Future} {Multidisciplinary} {Society}},
	volume = {11},
	issn = {1756-8757, 1756-8765},
	shorttitle = {Welcome to {Cognitive} {Science}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12471},
	doi = {10.1111/tops.12471},
	language = {English},
	number = {4},
	urldate = {2021-06-13},
	journal = {Topics in Cognitive Science},
	author = {Gray, Wayne D.},
	month = oct,
	year = {2019},
	pages = {838--844},
	file = {Gray-2019-Welcome_to_Cognitive_Science.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2019/Gray-2019-Welcome_to_Cognitive_Science.pdf:application/pdf;Gray - 2019 - Welcome to Cognitive Science The Once and Future .pdf:/home/paolo/zotero_data/storage/BBFGJPMK/Gray - 2019 - Welcome to Cognitive Science The Once and Future .pdf:application/pdf},
}

@misc{Mind,
	title = {Mind},
	url = {https://www.britannica.com/topic/mind},
	abstract = {Mind, in the Western tradition, the complex of faculties involved in perceiving, remembering, considering, evaluating, and deciding. Mind is in some sense reflected in such occurrences as sensations, perceptions, emotions, memory, desires, various types of reasoning, motives, choices, traits of},
	language = {English},
	urldate = {2021-06-14},
	file = {Snapshot:/home/paolo/zotero_data/storage/SB8X4DVG/mind.html:text/html},
}

@misc{InternationalEncyclopediaSocial,
	title = {International {Encyclopedia} of {Social} \& {Behavioral} {Sciences} - 1st {Edition}},
	url = {https://www.elsevier.com/books/international-encyclopedia-of-social-and-behavioral-sciences/smelser/978-0-08-043076-8},
	urldate = {2021-06-14},
	file = {International Encyclopedia of Social & Behavioral Sciences - 1st Edition:/home/paolo/zotero_data/storage/K3GF6WBT/978-0-08-043076-8.html:text/html;International_Encyclopedia_of_Social_&_Behavioral_Sciences_-_1st_Edition.pdf:/home/paolo/zotero_data/storage/QDIGX4HS/International_Encyclopedia_of_Social_&_Behavioral_Sciences_-_1st_Edition.pdf:application/pdf},
}

@book{rumelhartExplorationsCognition1975,
	title = {Explorations in {Cognition}},
	isbn = {978-0-7167-0736-3},
	publisher = {Freeman},
	author = {Rumelhart, David E. and Norman, Donald A.},
	year = {1975},
}

@book{gardnerMindNewScience1987,
	title = {The {Mind}'s {New} {Science}: {A} {History} of the {Cognitive} {Revolution}},
	shorttitle = {The {Mind}'s {New} {Science}},
	abstract = {The first full-scale history of cognitive science, this work addresses a central issue: What is the nature of knowledge?},
	author = {Gardner, Howard E.},
	month = jun,
	year = {1987},
}

@book{bodenMindMachineHistory2008,
	title = {Mind as {Machine}: {A} {History} of {Cognitive} {Science}},
	isbn = {978-0-19-954316-8},
	shorttitle = {Mind as {Machine}},
	abstract = {The development of cognitive science is one of the most remarkable and fascinating intellectual achievements of the modern era. The quest to understand the mind is as old as recorded human thought; but the progress of modern science has offered new methods and techniques which have revolutionized this enquiry. Oxford University Press now presents a masterful history of cognitive science, told by one of its most eminent practitioners. Cognitive science is the project of understanding the mind by modeling its workings. Psychology is its heart, but it draws together various adjoining fields of research, including artificial intelligence; neuroscientific study of the brain; philosophical investigation of mind, language, logic, and understanding; computational work on logic and reasoning; linguistic research on grammar, semantics, and communication; and anthropological explorations of human similarities and differences. Each discipline, in its own way, asks what the mind is, what it does, how it works, how it developed - how it is even possible. The key distinguishing characteristic of cognitive science, Boden suggests, compared with older ways of thinking about the mind, is the notion of understanding the mind as a kind of machine. She traces the origins of cognitive science back to Descartes's revolutionary ideas, and follows the story through the eighteenth and nineteenth centuries, when the pioneers of psychology and computing appear. Then she guides the reader through the complex interlinked paths along which the study of the mind developed in the twentieth century. Cognitive science, in Boden's broad conception, covers a wide range of aspects of mind: not just 'cognition' in the sense of knowledge or reasoning, but emotion, personality, social communication, and even action. In each area of investigation, Boden introduces the key ideas and the people who developed them. No one else could tell this story as Boden can: she has been an active participant in cognitive science since the 1960s, and has known many of the key figures personally. Her narrative is written in a lively, swift-moving style, enriched by the personal touch of someone who knows the story at first hand. Her history looks forward as well as back: it is her conviction that cognitive science today–and tomorrow–cannot be properly understood without a historical perspective. Mind as Machine will be a rich resource for anyone working on the mind, in any academic discipline, who wants to know how our understanding of our mental activities and capacities has developed.},
	language = {English},
	publisher = {Clarendon Press},
	author = {Boden, Margaret A.},
	month = jun,
	year = {2008},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Computers / Artificial Intelligence / General, Philosophy / Mind \& Body},
}

@book{sheehyCognitiveScience1995,
	title = {Cognitive {Science}},
	abstract = {Cognitive science explores intelligence and intelligent systems. Several disciplines, including psychology, philosophy, linguistics, and the neurosciences, have a well-established interest in these topics. An attempt to organize and unify views of thought developed within these distinct disciplines, cognitive science is concerned with the construction of abstract theory of intelligent processes, the investigation of human and animal intelligence, and a discussion of computational principles that underlie the organization and behavior of computer programs. This three volume set presents a careful selection of the most important articles on cognitive science, divided into the following areas: Foundational Issues Conceptualization, Learning, \& Memory Representation Problem Solving \& Understanding Visual Perception Comprehension Production Articles in these volumes have been drawn from various books and from the following journals: Science, Psychological Bulletin, The Psychology of Computer Vision, Psychological Review, Cognitive Science, Computers and Thought, Artificial Intelligence, Computers and Biomedical Research, Cognitive Psychology, Cognition, Language and Speech, and Computational Linguistics},
	author = {Sheehy, Noel and Chapman, Antony J.},
	month = sep,
	year = {1995},
}

@book{booleLawsThought2013,
	title = {The laws of thought},
	abstract = {This historic book may have numerous typos and missing text. Purchasers can usually download a free scanned copy of the original book (without typos) from the publisher. Not indexed. Not illustrated. 1916 edition. Excerpt: ...substance and connexion, dwelling only on certain por-I98tions of it which are of a more complex character than the others, and afford better illustrations of the method of this work. In Prop. IV. it is shown that the substance or essence of the self-existent being is incomprehensible. The tenor of the reasoning employed is, that we are ignorant of the essential nature of all other things,–much more, then, of the essence of the self-existent being. In Prop. V. it is contended that "though the substance or essence of the self-existent being is itself absolutely incomprehensible to us, yet many of the essential attributes of his nature are strictly demonstrable, as well as his existence." In Prop. VI. it is argued that "the self-existent being must of necessity be infinite and omnipresent "; and it is contended that his infinity must be "an infinity of fulness as well as of immensity." The ground upon which the demonstration proceeds is, that an absolute necessity of existence must be independent of time, place, and circumstance, free from limitation, and therefore excluding all imperfection. And hence it is inferred that the self-existent being must be " a most simple, unchangeable, incorruptible being, without parts, figure, motion, or any other such properties as we find in matter." The premises actually employed may be exhibited as follows: I. If a finite being is self-existent, it is a contradiction to suppose it not to exist. 2. A finite being may, without contradiction, be absent from one place. ' 3. That which may without contradiction be absent from one place may without contradiction be absent from all places. 4. That which may without-contradiction be absent from all places may...},
	author = {Boole, George},
	month = sep,
	year = {2013},
}

@incollection{monkMathematicsBooleanAlgebra2018,
	edition = {Fall 2018},
	title = {The {Mathematics} of {Boolean} {Algebra}},
	url = {https://plato.stanford.edu/archives/fall2018/entries/boolalg-math/},
	abstract = {Boolean algebra is the algebra of two-valued logic with onlysentential connectives, or equivalently of algebras of sets underunion and complementation. The rigorous concept is that of a certainkind of algebra, analogous to the mathematical notion of agroup. This concept has roots and applications in logic(Lindenbaum-Tarski algebras and model theory), set theory (fields ofsets), topology (totally disconnected compact Hausdorff spaces),foundations of set theory (Boolean-valued models), measure theory(measure algebras), functional analysis (algebras of projections),and ring theory (Boolean rings). The study of Boolean algebrashas several aspects: structure theory, model theory of Booleanalgebras, decidability and undecidability questions for the class ofBoolean algebras, and the indicated applications. In addition,although not explained here, there are connections to other logics,subsumption as a part of special kinds of algebraic logic, finiteBoolean algebras and switching circuit theory, and Booleanmatrices.},
	urldate = {2021-06-15},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Monk, J. Donald},
	editor = {Zalta, Edward N.},
	year = {2018},
	keywords = {algebra, algebra of logic tradition, logic: algebraic propositional, model theory, set theory, Boole, George},
	file = {SEP - Snapshot:/home/paolo/zotero_data/storage/9UIUPP3G/boolalg-math.html:text/html},
}

@article{mccullochLogicalCalculusIdeas1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://link.springer.com/article/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {English},
	number = {4},
	urldate = {2021-06-16},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	note = {tex.copyright: 1943 The University of Chicago Press},
	pages = {115--133},
	file = {Snapshot:/home/paolo/zotero_data/storage/JKBFZ6V8/10.html:text/html;McCulloch_Pitts-1943-A_logical_calculus_of_the_ideas_immanent_in_nervous_activity.pdf:/home/paolo/zotero_data/storage/353UPNUM/McCulloch_Pitts-1943-A_logical_calculus_of_the_ideas_immanent_in_nervous_activity.pdf:application/pdf},
}

@article{turingComputableNumbersApplication1937,
	title = {On {Computable} {Numbers}, with an {Application} to the {Entscheidungsproblem}},
	volume = {s2-42},
	issn = {1460-244X},
	url = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s2-42.1.230},
	doi = {10.1112/plms/s2-42.1.230},
	language = {English},
	number = {1},
	urldate = {2021-06-16},
	journal = {Proceedings of the London Mathematical Society},
	author = {Turing, A. M.},
	year = {1937},
	note = {tex.copyright: © 1937 London Mathematical Society},
	pages = {230--265},
	file = {Snapshot:/home/paolo/zotero_data/storage/34EMMDYV/s2-42.1.html:text/html;Turing-1937-On_Computable_Numbers,_with_an_Application_to_the_Entscheidungsproblem.pdf:/home/paolo/zotero_data/storage/AMCLDCZN/Turing-1937-On_Computable_Numbers,_with_an_Application_to_the_Entscheidungsproblem.pdf:application/pdf},
}

@article{steerCyberneticsCircularCausal1952,
	title = {Cybernetics: {Circular} {Causal} and {Feedback} {Mechanisms} in {Biological} and {Social} {Systems}. {Transactions} of the {Seventh} {Conference}, {March} 23-24, 1950, {New} {York}. {Heinz} von {Foerster}, {Ed}. {New} {York}: {Josiah} {Macy}, {Jr}. {Foundation}, 1951. 251 pp. \$3.50},
	volume = {115},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Cybernetics},
	url = {https://science.sciencemag.org/content/115/2978/100.1},
	doi = {10.1126/science.115.2978.100},
	language = {English},
	number = {2978},
	urldate = {2021-06-16},
	journal = {Science},
	author = {Steer, M. D.},
	month = jan,
	year = {1952},
	note = {tex.copyright: Copyright © 1952 by the American Association for the Advancement of Science},
	pages = {100--100},
	file = {Snapshot:/home/paolo/zotero_data/storage/NK6RYEFF/100.1.html:text/html},
}

@article{anokhinProblemsCentrePeriphery1935,
	title = {Problems of centre and periphery in the physiology of nervous activity},
	journal = {Gorki, Gozizdat},
	author = {Anokhin, P. K.},
	year = {1935},
}

@inproceedings{wienerCyberneticsControlCommunication1961,
	title = {Cybernetics : {Control} and {Communication} in the {Animal} and the {Machine} –2nd. ed},
	shorttitle = {Cybernetics},
	url = {http://cumincad.scix.net/cgi-bin/works/Show&_id=caadria2010_000&sort=DEFAULT&search=series:caadria/Show?_id=4e2e&sort=DEFAULT&search=%2Fseries%3A%22CADline%22&hits=808},
	abstract = {It is the author belief that cybernetics should be considered not merely as a program to be carried out in the future, but as an existing science. In this book the author's intention is to express and to exemplify his ideas on cybernetics and to display some of his personal philosophical reflections},
	urldate = {2021-06-16},
	booktitle = {212 p. {Cambridge}, {Mass}.: {The} {MIT} press, 1961. includes index},
	publisher = {CUMINCAD},
	author = {Wiener, Norbert},
	year = {1961},
	file = {Snapshot:/home/paolo/zotero_data/storage/IRBCW6CA/Show.html:text/html},
}

@article{rosenbluethBehaviorPurposeTeleology1943,
	title = {Behavior, {Purpose} and {Teleology}},
	volume = {10},
	issn = {0031-8248},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/286788},
	doi = {10.1086/286788},
	number = {1},
	urldate = {2021-06-16},
	journal = {Philosophy of Science},
	author = {Rosenblueth, Arturo and Wiener, Norbert and Bigelow, Julian},
	month = jan,
	year = {1943},
	pages = {18--24},
	file = {Snapshot:/home/paolo/zotero_data/storage/BJ2H3PXD/286788.html:text/html},
}

@incollection{copelandModernHistoryComputing2020,
	edition = {Winter 2020},
	title = {The {Modern} {History} of {Computing}},
	url = {https://plato.stanford.edu/archives/win2020/entries/computing-history/},
	abstract = {Historically, computers were human clerks who calculated in accordancewith effective methods. These human computers did the sorts ofcalculation nowadays carried out by electronic computers, and manythousands of them were employed in commerce, government, and researchestablishments. The term computing machine, used increasinglyfrom the 1920s, refers to any machine that does the work of a humancomputer, i.e., any machine that calculates in accordance witheffective methods. During the late 1940s and early 1950s, with theadvent of electronic computing machines, the phrase ‘computingmachine’ gradually gave way simply to ‘computer’,initially usually with the prefix ‘electronic’ or‘digital’. This entry surveys the history of thesemachines.},
	urldate = {2021-06-17},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Copeland, B. Jack},
	editor = {Zalta, Edward N.},
	year = {2020},
	keywords = {computability and complexity, recursive functions, Turing machines, Turing, Alan},
	file = {SEP - Snapshot:/home/paolo/zotero_data/storage/HTCYKGDZ/computing-history.html:text/html},
}

@incollection{grahamBehaviorism2019,
	edition = {Spring 2019},
	title = {Behaviorism},
	url = {https://plato.stanford.edu/archives/spr2019/entries/behaviorism/},
	abstract = {It has sometimes been said that “behave is what organismsdo.” Behaviorism is built on this assumption, and its goal is topromote the scientific study of behavior. The behavior, in particular,of individual organisms. Not of social groups. Not of cultures. Butof persons and animals. , In this entry I consider different types of behaviorism. I outlinereasons for and against being a behaviorist. I consider contributionsof behaviorism to the study of behavior. Special attention is given tothe so-called “radical behaviorism” of B. F. Skinner(1904–90). Skinner is given special (not exclusive) attentionbecause he is the behaviorist who has received the most attention fromphilosophers, fellow scientists and the public at large. Generallessons can also be learned from Skinner about the conduct ofbehavioral science in general. The entry describes those lessons.},
	urldate = {2021-06-17},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Graham, George},
	editor = {Zalta, Edward N.},
	year = {2019},
	file = {SEP - Snapshot:/home/paolo/zotero_data/storage/CRSFSQFS/behaviorism.html:text/html},
}

@article{wagemansCenturyGestaltPsychology2012,
	title = {A {Century} of {Gestalt} {Psychology} in {Visual} {Perception} {I}. {Perceptual} {Grouping} and {Figure}-{Ground} {Organization}},
	volume = {138},
	issn = {0033-2909},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3482144/},
	doi = {10.1037/a0029333},
	abstract = {In 1912, Max Wertheimer published his paper on phi motion, widely recognized as the start of Gestalt psychology. Because of its continued relevance in modern psychology, this centennial anniversary is an excellent opportunity to take stock of what Gestalt psychology has offered and how it has changed since its inception. We first introduce the key findings and ideas in the Berlin school of Gestalt psychology, and then briefly sketch its development, rise, and fall. Next, we discuss its empirical and conceptual problems, and indicate how they are addressed in contemporary research on perceptual grouping and figure-ground organization. In particular, we review the principles of grouping, both classical (e.g., proximity, similarity, common fate, good continuation, closure, symmetry, parallelism) and new (e.g., synchrony, common region, element and uniform connectedness), and their role in contour integration and completion. We then review classic and new image-based principles of figure-ground organization, how it is influenced by past experience and attention, and how it relates to shape and depth perception. After an integrated review of the neural mechanisms involved in contour grouping, border-ownership, and figure-ground perception, we conclude by evaluating what modern vision science has offered compared to traditional Gestalt psychology, whether we can speak of a Gestalt revival, and where the remaining limitations and challenges lie. A better integration of this research tradition with the rest of vision science requires further progress regarding the conceptual and theoretical foundations of the Gestalt approach, which will be the focus of a second review paper.},
	number = {6},
	urldate = {2021-06-17},
	journal = {Psychological bulletin},
	author = {Wagemans, Johan and Elder, James H. and Kubovy, Michael and Palmer, Stephen E. and Peterson, Mary A. and Singh, Manish and von der Heydt, Rüdiger},
	month = nov,
	year = {2012},
	pmid = {22845751},
	note = {tex.pmcid: PMC3482144},
	pages = {1172--1217},
	file = {Wagemans_et_al-2012-A_Century_of_Gestalt_Psychology_in_Visual_Perception_I.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2012/Wagemans_et_al-2012-A_Century_of_Gestalt_Psychology_in_Visual_Perception_I.pdf:application/pdf},
}

@article{hullGoalAttractionDirecting1931,
	title = {Goal attraction and directing ideas conceived as habit phenomena},
	volume = {38},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0071442},
	abstract = {The author argues that the drive stimulus which acts adequately for the random seeking reactions of a hungry organism is insufficient alone to produce the integration of complex behavior sequences such as are involved in maze learning. In addition the presence of the reward causes anticipatory goal reactions to accompany the sequence leading to the full overt goal reaction. The kinesthetic stimulation resulting from these persistent anticipatory actions produces a second stimulus which persists along with the drive stimulus and depends upon it. Certain facts are thereby made understandable, such as the facts that withholding the usual reward causes disintegration of one particular habit sequence without preventing the pursuit of alternative sequences based on the same drive, and the fact that the substitution of one reward for another in the same situation causes disintegration. The author believes that anticipatory goal reactions are the physical substance of purposive ideas and the basis of what is known as ideo-motor action. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Psychological Review},
	author = {Hull, C. L.},
	year = {1931},
	keywords = {Animal Ethology, Animal Learning, Animal Motivation, Maze Learning},
	pages = {487--506},
	file = {Snapshot:/home/paolo/zotero_data/storage/HBWQB4VF/1932-00166-001.html:text/html},
}

@article{tolmanCognitiveMapsRats1948,
	title = {Cognitive maps in rats and men},
	volume = {55},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0061626},
	abstract = {This paper is devoted to a description of experiments with rats, mostly at the author's laboratory, and to indicating the significance of these findings on rats for the clinical behavior of men. While all students agree as to the facts reported, they disagree on theory and explanation. 5 kinds of experiments (latent learning, vicarious trial and error, searching for the stimulus, hypotheses, and spatial orientation) are described and discussed. The conditions which favor (cognitive) narrow strip-maps and which favor broad comprehensive maps in rats and in men are considered. Narrow strip-maps seem to be indicated by (1) a damaged brain, (2) an inadequate arrangement of environmentally presented cues, (3) a surplus of repetitions on the original trained-on path, and (4) the presence of too strongly frustrating conditions. The fourth point is elaborated. It is contended that some of the psychological mechanisms which clinical psychologists and other students of personality have uncovered as factors underlying many individual and social maladjustments can be interpreted "as narrowings of our cognitive maps due to too strong motivations or to too intense frustrations." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Psychological Review},
	author = {Tolman, Edward C.},
	year = {1948},
	keywords = {Cognitive Maps, Comparative Psychology, Personality, Rats, Spatial Orientation (Perception)},
	pages = {189--208},
	file = {Snapshot:/home/paolo/zotero_data/storage/ISEKHI2Z/1949-00103-001.html:text/html},
}

@book{skinnerBehaviorism1974,
	address = {Oxford, England},
	series = {About behaviorism},
	title = {About behaviorism},
	isbn = {978-0-394-49201-8},
	abstract = {Presents Skinner's explanation of the theories propounded in his book Beyond Freedom and Dignity defining, analyzing, and defending his highly controversial views on behaviorism. Among the topics discussed are the causes of behavior; innate, operant, and verbal behavior; thinking and knowing; and emotion and the sense of self. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Alfred A. Knopf},
	author = {Skinner, B. F.},
	year = {1974},
	keywords = {Behaviorism},
	file = {Snapshot:/home/paolo/zotero_data/storage/2ZEEGTYL/1975-00035-000.html:text/html},
}

@article{tremblayBrocaWernickeAre2016,
	title = {Broca and {Wernicke} are dead, or moving past the classic model of language neurobiology},
	volume = {162},
	issn = {0093-934X},
	url = {https://www.sciencedirect.com/science/article/pii/S0093934X16300475},
	doi = {10.1016/j.bandl.2016.08.004},
	abstract = {With the advancement of cognitive neuroscience and neuropsychological research, the field of language neurobiology is at a cross-roads with respect to its framing theories. The central thesis of this article is that the major historical framing model, the Classic “Wernicke-Lichtheim-Geschwind” model, and associated terminology, is no longer adequate for contemporary investigations into the neurobiology of language. We argue that the Classic model (1) is based on an outdated brain anatomy; (2) does not adequately represent the distributed connectivity relevant for language, (3) offers a modular and “language centric” perspective, and (4) focuses on cortical structures, for the most part leaving out subcortical regions and relevant connections. To make our case, we discuss the issue of anatomical specificity with a focus on the contemporary usage of the terms “Broca’s and Wernicke’s area”, including results of a survey that was conducted within the language neurobiology community. We demonstrate that there is no consistent anatomical definition of “Broca’s and Wernicke’s Areas”, and propose to replace these terms with more precise anatomical definitions. We illustrate the distributed nature of the language connectome, which extends far beyond the single-pathway notion of arcuate fasciculus connectivity established in Geschwind’s version of the Classic Model. By illustrating the definitional confusion surrounding “Broca’s and Wernicke’s areas”, and by illustrating the difficulty integrating the emerging literature on perisylvian white matter connectivity into this model, we hope to expose the limits of the model, argue for its obsolescence, and suggest a path forward in defining a replacement.},
	language = {English},
	urldate = {2021-06-17},
	journal = {Brain and Language},
	author = {Tremblay, Pascale and Dick, Anthony Steven},
	month = nov,
	year = {2016},
	keywords = {Arcuate fasciculus, Broca’s area, Language connectome, Language neurobiology, Wernicke’s area},
	pages = {60--71},
	file = {Tremblay_Dick-2016-Broca_and_Wernicke_are_dead,_or_moving_past_the_classic_model_of_language.pdf:/home/paolo/zotero_data/storage/V3VL88PS/Tremblay_Dick-2016-Broca_and_Wernicke_are_dead,_or_moving_past_the_classic_model_of_language.pdf:application/pdf;ScienceDirect Snapshot:/home/paolo/zotero_data/storage/Q33JVP4V/S0093934X16300475.html:text/html},
}

@misc{ComputerResurrectionIssue2012,
	title = {Computer {Resurrection} {Issue} 20},
	url = {https://web.archive.org/web/20120109142655/http://www.cs.man.ac.uk/CCS/res/res20.htm#d#d},
	urldate = {2021-06-17},
	month = jan,
	year = {2012},
	file = {Snapshot:/home/paolo/zotero_data/storage/A7TK649X/res20.html:text/html},
}

@article{nyquistCertainTopicsTelegraph1928,
	title = {Certain {Topics} in {Telegraph} {Transmission} {Theory}},
	volume = {47},
	issn = {2330-9431},
	doi = {10.1109/T-AIEE.1928.5055024},
	abstract = {The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.},
	number = {2},
	journal = {Transactions of the American Institute of Electrical Engineers},
	author = {Nyquist, H.},
	month = apr,
	year = {1928},
	keywords = {Costs, Circuits, Distortion, Equalizers, Frequency conversion, Interference, Shape, Steady-state, Telegraphy, Telephony},
	pages = {617--644},
	file = {IEEE Xplore Abstract Record:/home/paolo/zotero_data/storage/7UHKSRX3/5055024.html:text/html},
}

@article{watsonUnverbalizedHumanBehavior1924,
	title = {The {Unverbalized} in {Human} {Behavior}},
	volume = {31},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0071569},
	abstract = {The organization of bodily habits may or may not be paralleled by verbal organization. In infants too young to talk a great deal of organization goes on. The manual habits of three-year-and-under infants are unverbalized and the only way to test "memory" is to place infants in a situation where bodily organization can be exhibited. The weakness of the Freudian assumption of the unconscious is this: the infant never had verbal organization for the experiences supposed to be repressed in the unconscious. Infancy turns out to be a wholly natural state of being. There is another field in which unverbalized organization goes on. Not only in infancy but throughout life our visceral system is subject to the laws of habit formation. From Psych Bulletin 22:12:00842. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Psychological Review},
	author = {Watson, J. B.},
	year = {1924},
	keywords = {Habits, Infant Development, Memory},
	pages = {273--280},
	file = {Snapshot:/home/paolo/zotero_data/storage/4YM6QGQT/1926-08209-001.html:text/html},
}

@article{millerCognitiveRevolutionHistorical2003,
	title = {The cognitive revolution: a historical perspective},
	volume = {7},
	issn = {13646613},
	shorttitle = {The cognitive revolution},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661303000299},
	doi = {10.1016/S1364-6613(03)00029-9},
	language = {English},
	number = {3},
	urldate = {2021-06-18},
	journal = {Trends in Cognitive Sciences},
	author = {Miller, George A},
	month = mar,
	year = {2003},
	pages = {141--144},
	file = {Miller-2003-The_cognitive_revolution.pdf:/home/paolo/zotero_data/storage/A7KUHWVH/Miller-2003-The_cognitive_revolution.pdf:application/pdf},
}

@book{millerMagicalNumberSeven2020,
	title = {The magical number seven, plus-or-minus two or some limits on our capacity for processing information},
	publisher = {University of California Press},
	author = {Miller, George A.},
	year = {2020},
	file = {Snapshot:/home/paolo/zotero_data/storage/8357ZAB2/html.html:text/html;Miller-2020-The_magical_number_seven,_plus-or-minus_two_or_some_limits_on_our_capacity_for.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2020/Miller-2020-The_magical_number_seven,_plus-or-minus_two_or_some_limits_on_our_capacity_for.pdf:application/pdf},
}

@book{brunerStudyThinking1986,
	title = {A study of thinking},
	publisher = {Transaction publishers},
	author = {Bruner, Jerome Seymour and Austin, George Allen},
	year = {1986},
	file = {Snapshot:/home/paolo/zotero_data/storage/DPRMDFJX/books.html:text/html},
}

@book{brunerStudyThinking1956,
	address = {New York},
	title = {A study of thinking},
	language = {English},
	publisher = {Wiley},
	author = {Bruner, Jerome S.},
	year = {1956},
	keywords = {Thought and thinking.},
}

@article{maccorquodaleChomskyReviewSkinner1970,
	title = {On {Chomsky}'s review of {Skinner}'s {Verbal} behavior},
	volume = {13},
	number = {1},
	journal = {Journal of the experimental analysis of behavior},
	author = {MacCorquodale, Kenneth},
	year = {1970},
	pages = {83},
	file = {MacCorquodale-1970-On_Chomsky's_review_of_Skinner's_Verbal_behavior.pdf:/home/paolo/zotero_data/storage/3ZUANGDD/MacCorquodale-1970-On_Chomsky's_review_of_Skinner's_Verbal_behavior.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/UTAHCSB2/PMC1333660.html:text/html},
}

@book{skinnerVerbalBehavior1957,
	title = {Verbal behavior},
	publisher = {New York: Appleton-Century-Crofts},
	author = {Skinner, Burrhus Frederic},
	year = {1957},
	file = {Skinner-1957-Verbal_behavior.pdf:/home/paolo/zotero_data/storage/Z2LSGY4R/Skinner-1957-Verbal_behavior.pdf:application/pdf},
}

@book{chomskyReviewBFSkinner2013,
	title = {A {Review} of {BF} {Skinner}’s {Verbal} {Behavior}},
	publisher = {Harvard University Press},
	author = {Chomsky, Noam},
	year = {2013},
	file = {Full Text:/home/paolo/zotero_data/storage/NG2T57AR/chomsky.html:text/html;Snapshot:/home/paolo/zotero_data/storage/HSQGNYFN/html.html:text/html},
}

@book{newmeyerPoliticsLinguistics1986,
	title = {The politics of linguistics},
	isbn = {978-0-226-57720-3},
	url = {https://dialnet.unirioja.es/servlet/libro?codigo=605873},
	abstract = {Autoría: Frederick J. Newmeyer. Año de publicación: 1986. Libro en Dialnet.},
	language = {English},
	urldate = {2021-06-18},
	author = {Newmeyer, Frederick J.},
	year = {1986},
	file = {Snapshot:/home/paolo/zotero_data/storage/PXMZ88PD/libro.html:text/html},
}

@book{chomskySyntacticStructures1985,
	address = {The Hague},
	edition = {14. printing},
	series = {Janua {Linguarum} {Series} minor},
	title = {Syntactic structures},
	isbn = {978-90-279-3385-0},
	language = {English},
	number = {4},
	publisher = {Mouton},
	author = {Chomsky, Noam},
	year = {1985},
	file = {Chomsky-1985-Syntactic_structures.pdf:/home/paolo/zotero_data/storage/MLSVTXTI/Chomsky-1985-Syntactic_structures.pdf:application/pdf},
}

@book{steinbergPsycholinguisticsLanguageMind2013,
	title = {Psycholinguistics: {Language}, {Mind} and {World}},
	isbn = {978-1-317-90056-6},
	shorttitle = {Psycholinguistics},
	abstract = {How do we learn to produce and comprehend speech? How does language relate to thought?This second edition of the successful text Psycholinguistics- Language, Mind and World considers the psychology of language as it relates to learning, mind and brain as well as various aspects of society and culture. Current issues and research topics are presented in an in-depth manner, although little or no specific knowledge of any topic is presupposed.The book is divided into four main parts: First Language Learning Second Language Learning Language, Mind and Brain Mental Grammar and Language Processing These four sections include chapters covering areas such as- deaf language education, first language acquisition and first language reading, second language acquisition, language teaching and the problems of bilingualism.Updated throughout, this new edition also considers and proposes new theories in psycholinguistics and linguistics, and introduces a new theory of grammar, Natural Grammar, which is the only current grammar that is based on the primacy of the psycholinguistic process of speech comprehension, derives speech production from that process. Written in an accessible and fluent style, Psycholinguistics- Language, Mind and World will be of interest to students, lecturers and researchers from linguistics, psychology, philosophy and second language teaching.},
	language = {English},
	publisher = {Routledge},
	author = {Steinberg, Danny D. and Nagata, Hiroshi and Aline, David P.},
	month = oct,
	year = {2013},
	keywords = {Language Arts \& Disciplines / General, Language Arts \& Disciplines / Linguistics / General, Language Arts \& Disciplines / Linguistics / Psycholinguistics},
}

@article{russellArtificialIntelligenceModern2002,
	title = {Artificial intelligence: a modern approach},
	shorttitle = {Artificial intelligence},
	author = {Russell, Stuart and Norvig, Peter},
	year = {2002},
	file = {Russell_Norvig-2002-Artificial_intelligence.pdf:/home/paolo/zotero_data/storage/BWLM2US9/Russell_Norvig-2002-Artificial_intelligence.pdf:application/pdf},
}

@article{newellComputerScienceEmpirical1976,
	title = {Computer science as empirical inquiry: symbols and search},
	volume = {19},
	issn = {0001-0782},
	shorttitle = {Computer science as empirical inquiry},
	url = {https://doi.org/10.1145/360018.360022},
	doi = {10.1145/360018.360022},
	abstract = {Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine—not just the hardware, but the programmed, living machine—is the organism we study.},
	number = {3},
	urldate = {2021-06-19},
	journal = {Communications of the ACM},
	author = {Newell, Allen and Simon, Herbert A.},
	month = mar,
	year = {1976},
	keywords = {artificial intelligence, cognition, computer science, empirical, heuristics, list processing, problem solving, science, search, symbols, Turing},
	pages = {113--126},
	file = {Newell_Simon-1976-Computer_science_as_empirical_inquiry.pdf:/home/paolo/zotero_data/storage/4BLSIQTR/Newell_Simon-1976-Computer_science_as_empirical_inquiry.pdf:application/pdf},
}

@incollection{widrowAssociativeStorageRetrieval1962,
	address = {Boston, MA},
	title = {Associative {Storage} and {Retrieval} of {Digital} {Information} in {Networks} of {Adaptive} “{Neurons}”},
	isbn = {978-1-4684-1716-6},
	url = {https://doi.org/10.1007/978-1-4684-1716-6_25},
	abstract = {An adaptive logical element, called the ADALINE “neuron,” which consists of a set of variable weights, a threshold, and adaptation machinery for automatically adjusting its weights, has been described previously.* Proofs of convergence of its learning processes and derivations of learning rates have been made. It has been demonstrated analytically and empirically that a single ADALINE can be “trained” to recognize geometric patterns, perform logical functions, and store digital information.Machines capable of being trained to solve multistage decision problems require associative memories which permit the abstraction of previous experiences similar to current problem situations. Networks of ADALINEs can perform the associative memory function. Three types of networks are considered.One system stores incident information in available memory registers. Classification is accomplished in the read-out process after storage by means of adaptive-neuron classifiers attached to each register. Associative retrieval is accomplished by simultaneously training the classifiers, then causing all registers to transpond to a central control if their contents fall in the desired class.Another form of memory allows some classification of information to take place during storage, and some during retrieval. A tree-like structure, connected as a traffic control system, can be trained to route input information according to class into many storage bins, where it is recorded and possibly further classified. At each juncture in the tree, a neuron acts like an “adaptive traffic cop,” controlling (gating) the choice of propagation path. After training, each class of input pattern excites a characteristic connection path or “trace” through the tree.In a third memory system, information is stored directly in a bank of adaptive neurons and is classified as it is stored. This system is trained to produce a set of D binary output digits in response to an input set of N binary digits, and has the ability to generalize in that input stimuli that are similar to the training experiences evoke the same responses. The number of experiences required to train the neuron memory equals several times the product ND. This type of memory tends to forget previous experiences exponentially. A set of experiences, in order to be completely remembered, has to be repeated over and over again until it “sinks in,” i.e., until the training process converges. The similarity to animal memory is striking.},
	language = {English},
	urldate = {2021-06-19},
	booktitle = {Biological {Prototypes} and {Synthetic} {Systems}: {Volume} 1 {Proceedings} of the {Second} {Annual} {Bionics} {Symposium} sponsored by {Cornell} {University} and the {General} {Electric} {Company}, {Advanced} {Electronics} {Center}, held at {Cornell} {University}, {August} 30–{September} 1, 1961},
	publisher = {Springer US},
	author = {Widrow, Bernard and Hoff, Marcian E.},
	editor = {Bernard, Eugene E. and Kare, Morley R.},
	year = {1962},
	doi = {10.1007/978-1-4684-1716-6_25},
	pages = {160--160},
	file = {Widrow_Hoff-1962-Associative_Storage_and_Retrieval_of_Digital_Information_in_Networks_of.pdf:/home/paolo/zotero_data/storage/R3H33RBD/Widrow_Hoff-1962-Associative_Storage_and_Retrieval_of_Digital_Information_in_Networks_of.pdf:application/pdf},
}

@article{friedbergLearningMachinePart1958,
	title = {A {Learning} {Machine}: {Part} {I}},
	volume = {2},
	issn = {0018-8646},
	shorttitle = {A {Learning} {Machine}},
	doi = {10.1147/rd.21.0002},
	abstract = {Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.},
	number = {1},
	journal = {IBM Journal of Research and Development},
	author = {Friedberg, R. M.},
	month = jan,
	year = {1958},
	pages = {2--13},
	file = {IEEE Xplore Abstract Record:/home/paolo/zotero_data/storage/XFQE8S22/5392654.html:text/html},
}

@article{ayllonPsychiatricNurseBehavioral1959,
	title = {The psychiatric nurse as a behavioral engineer},
	volume = {2},
	issn = {0022-5002},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1403907/},
	doi = {10.1901/jeab.1959.2-323},
	number = {4},
	urldate = {2021-06-20},
	journal = {Journal of the Experimental Analysis of Behavior},
	author = {Ayllon, Teodoro and Michael, Jack},
	month = oct,
	year = {1959},
	pmid = {13795356},
	note = {tex.pmcid: PMC1403907},
	pages = {323--334},
	file = {Ayllon_Michael-1959-The_psychiatric_nurse_as_a_behavioral_engineer.pdf:/home/paolo/zotero_data/storage/UZU26CPK/Ayllon_Michael-1959-The_psychiatric_nurse_as_a_behavioral_engineer.pdf:application/pdf},
}

@incollection{sperlingInformationAvailableBrief1960,
	title = {The information available in brief visual presentations},
	url = {http://www.cogsci.uci.edu/~whipl/staff/sperling/PDFs/Sperling_PsychMonogr_1960.pdf},
	urldate = {2021-06-20},
	booktitle = {Psychological {Monographs}: {General} and {Applied}},
	author = {Sperling, George},
	year = {1960},
	pages = {1--29},
	file = {Sperling-1960-The_information_available_in_brief_visual_presentations.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1960/Sperling-1960-The_information_available_in_brief_visual_presentations.pdf:application/pdf},
}

@article{petersonShorttermRetentionIndividual1959,
	title = {Short-term retention of individual verbal items},
	volume = {58},
	issn = {0022-1015(Print)},
	doi = {10.1037/h0049234},
	abstract = {The investigation is concerned with individual items instead of lists. "Forgetting over intervals measured in seconds was found. The course of retention after a single presentation was related to a statistical model. Forgetting was found to progress at differential rates dependent on the amount of controlled rehearsal of the stimulus. A portion of the improvement in recall with repetitions was assigned to serial learning within the item, but a second kind of learning was also found." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Journal of Experimental Psychology},
	author = {Peterson, Lloyd and Peterson, Margaret Jean},
	year = {1959},
	keywords = {Retention, Serial Learning, Short Term Memory},
	pages = {193--198},
	file = {Snapshot:/home/paolo/zotero_data/storage/96FJZ29I/1960-05499-001.html:text/html},
}

@article{waughPrimaryMemory1965,
	title = {Primary memory},
	volume = {72},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0021797},
	abstract = {A model for short-term memory is described and evaluated. A variety of experimental data are shown to be consistent with the following statements. (a) Unrehearsed verbal stimuli tend to be quickly forgotten because they are interfered with by later items in a series and not because their traces decay in time. (b) Rehearsal may transfer an item from a very limited primary memory store to a larger and more stable secondary store. (c) A recently perceived item may be retained in both stores at the same time. The properties of these 2 independent memory systems can be separated by experimental and analytical methods. (30 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Review},
	author = {Waugh, Nancy C. and Norman, Donald A.},
	year = {1965},
	keywords = {Short Term Memory, Memory Trace, Verbal Stimuli},
	pages = {89--104},
	file = {Snapshot:/home/paolo/zotero_data/storage/DWTJ52L3/1965-09429-001.html:text/html},
}

@book{jamesPrinciplesPsychology1890,
	title = {The principles of psychology},
	volume = {1},
	number = {2},
	publisher = {Macmillan London},
	author = {James, William and Burkhardt, Frederick and Bowers, Fredson and Skrupskelis, Ignas K.},
	year = {1890},
	file = {Full Text:/home/paolo/zotero_data/storage/83T77YPL/principles.html:text/html;Snapshot:/home/paolo/zotero_data/storage/PECAJKAL/analysis-william-james-principles-psychology-macat-team.html:text/html},
}

@article{katzReleaseNeuralTransmitter1969,
	title = {The release of neural transmitter substances},
	url = {https://ci.nii.ac.jp/naid/10009658302/},
	urldate = {2021-06-20},
	journal = {Liverpool University Press},
	author = {KATZ, B.},
	year = {1969},
	pages = {5--39},
	file = {The release of neural transmitter substances Snapshot:/home/paolo/zotero_data/storage/5EL5M4SL/10009658302.html:text/html},
}

@article{katzIonicRequirementsSynaptic1967,
	title = {Ionic {Requirements} of {Synaptic} {Transmitter} {Release}},
	volume = {215},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/215651a0},
	doi = {10.1038/215651a0},
	abstract = {THE release of a transmitter substance by the nerve impulse depends on the presence of calcium, rather than sodium, on the outside of the axon membrane. There is increasing evidence that inward movement of calcium is one of the first steps in the “electro-secretory” process. This view has been strengthened by recent experiments with tetrodotoxin1–3, which eliminates the regenerative influx of sodium ions but does not interfere with the release of the transmitter by locally imposed depolarization provided calcium ions are present in the external solution.},
	language = {English},
	number = {5101},
	urldate = {2021-06-20},
	journal = {Nature},
	author = {Katz, B. and Miledi, R.},
	month = aug,
	year = {1967},
	note = {tex.copyright: 1967 Nature Publishing Group},
	pages = {651--651},
	file = {Katz_Miledi-1967-Ionic_Requirements_of_Synaptic_Transmitter_Release.pdf:/home/paolo/zotero_data/storage/ZLXZHYDM/Katz_Miledi-1967-Ionic_Requirements_of_Synaptic_Transmitter_Release.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/DIADKSSH/215651a0.html:text/html},
}

@article{cooperMentalRotationRandom1975,
	title = {Mental rotation of random two-dimensional shapes},
	volume = {7},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/0010028575900031},
	doi = {10.1016/0010-0285(75)90003-1},
	abstract = {Two experiments are reported in which Ss were required to determine whether a random, angular form, presented at any of a number of picture-plane orientations, was a “standard” or “reflected” version. Average time required to make this determination increased linearly with the angular departure of the form from a previously learned orientation. The slope and intercept of the reaction-time (RT) function were virtually constant, regardless of the perceptual complexity of the test form and the orientation selected for initial learning. When Ss were informed in advance as to the identity and the orientation of the upcoming test form and, further, were permitted to indicate when they were prepared for its external presentation, RT for determining the version of the form was constant for all test-form orientations. However, the time needed to prepare for the test-form presentation increased linearly with the angular departure of the form from the learned orientation. It is argued that the processes both of preparing for and of responding to a disoriented test form consist of the mental rotation of an image, and that both sorts of mental rotation (pre-stimulus and post-stimulus) are carried out at essentially the same constant rate.},
	language = {English},
	number = {1},
	urldate = {2021-06-20},
	journal = {Cognitive Psychology},
	author = {Cooper, Lynn A},
	month = jan,
	year = {1975},
	pages = {20--43},
	file = {ScienceDirect Snapshot:/home/paolo/zotero_data/storage/354WNWVW/0010028575900031.html:text/html},
}

@article{winogradProceduresRepresentationData1971,
	title = {Procedures as a {Representation} for {Data} in a {Computer} {Program} for {Understanding} {Natural} {Language}},
	url = {https://dspace.mit.edu/handle/1721.1/7095},
	abstract = {This paper describes a system for the computer understanding of English. The system answers questions, executes commands, and accepts information in normal English dialog. It uses semantic information and context to understand discourse and to disambiguate sentences. It combines a complete syntactic analysis of each sentence with a "heuristic understander" which uses different kinds of information about a sentence, other parts of the discourse, and general information about the world in deciding what the sentence means. It is based on the belief that a computer cannot deal reasonably with language unless it can "understand" the subject it is discussing. The program is given a detailed model of the knowledge needed by a simple robot having only a hand and an eye. We can give it instructions to manipulate toy objects, interrogate it about the scene, and give it information it will use in deduction. In addition to knowing the properties of toy objects, the program has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carry them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, and asking for clarification when its heuristic programs cannot understand a sentence through use of context and physical knowledge.},
	language = {English},
	urldate = {2021-06-21},
	author = {Winograd, Terry},
	month = jan,
	year = {1971},
	file = {Winograd-1971-Procedures_as_a_Representation_for_Data_in_a_Computer_Program_for_Understanding.pdf:/home/paolo/zotero_data/storage/UZRIFZCK/Winograd-1971-Procedures_as_a_Representation_for_Data_in_a_Computer_Program_for_Understanding.pdf:application/pdf},
}

@book{schankScriptsPlansGoals1977,
	address = {Oxford, England},
	series = {Scripts, plans, goals and understanding: {An} inquiry into human knowledge structures},
	title = {Scripts, plans, goals and understanding: {An} inquiry into human knowledge structures},
	isbn = {978-0-470-99033-9},
	shorttitle = {Scripts, plans, goals and understanding},
	abstract = {The nature of human knowledge and how this knowledge is used are explored from the points of view of both psychology and artificial intelligence. Among the topics analyzed are how concepts are structured in the human mind, how such concepts develop, how they are used in understanding and behavior, and how to program a computer so that it can understand and interact with the outside world. (6 p ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Lawrence Erlbaum},
	author = {Schank, Roger C. and Abelson, Robert P.},
	year = {1977},
	keywords = {Cognitive Processes, Computer Applications, Intelligence},
	file = {Snapshot:/home/paolo/zotero_data/storage/5MGRCV7C/1979-07723-000.html:text/html},
}

@phdthesis{wilenskyUnderstandingGoalbasedStories1978,
	address = {USA},
	type = {phd},
	title = {Understanding goal-based stories.},
	school = {Yale University},
	author = {Wilensky, Robert},
	year = {1978},
}

@article{schankComputerUnderstanding1981,
	title = {Inside {Computer} {Understanding}},
	url = {https://www.semanticscholar.org/paper/Inside-Computer-Understanding-Schank-Riesbeck/41a18aead56314c2eedd5efb5c7beef6c5212fbb},
	abstract = {Semantic Scholar extracted view of \&quot;Inside Computer Understanding\&quot; by R. Schank et al.},
	language = {English},
	urldate = {2021-06-21},
	journal = {undefined},
	author = {Schank, R. and Riesbeck, C.},
	year = {1981},
	file = {Snapshot:/home/paolo/zotero_data/storage/JFVZCUWF/41a18aead56314c2eedd5efb5c7beef6c5212fbb.html:text/html},
}

@book{brysonAppliedOptimalControl1969,
	address = {Waltham, Mass},
	series = {A {Blaisdell} book in the pure and applied sciences},
	title = {Applied optimal control: optimization, estimation, and control},
	shorttitle = {Applied optimal control},
	publisher = {Blaisdell Pub. Co},
	author = {Bryson, Arthur E. and Ho, Yu-Chi},
	year = {1969},
	keywords = {Control theory, Dynamic programming, Feedback control systems, Mathematical optimization},
}

@inproceedings{lecunTheoreticalFrameworkBackpropagation1988,
	title = {A theoretical framework for back-propagation},
	volume = {1},
	booktitle = {Proceedings of the 1988 connectionist models summer school},
	author = {LeCun, Yann and Touresky, D. and Hinton, G. and Sejnowski, T.},
	year = {1988},
	pages = {21--28},
	file = {LeCun_et_al-1988-A_theoretical_framework_for_back-propagation.pdf:/home/paolo/zotero_data/storage/M6MSEXAI/LeCun_et_al-1988-A_theoretical_framework_for_back-propagation.pdf:application/pdf},
}

@article{minskyPerceptronIntroductionComputational1969,
	title = {Perceptron: an introduction to computational geometry},
	volume = {19},
	shorttitle = {Perceptron},
	number = {88},
	journal = {The MIT Press, Cambridge, expanded edition},
	author = {Minsky, Marvin and Papert, Seymour},
	year = {1969},
	pages = {2},
}

@article{minskyFrameworkRepresentingKnowledge1975,
	title = {A framework for representing knowledge},
	author = {Minsky, M.},
	year = {1975},
	file = {Snapshot:/home/paolo/zotero_data/storage/IRWCCFZR/Show.html:text/html},
}

@article{andersonHumanAssociativeMemory1973,
	title = {Human {Associative} {Memory}},
	abstract = {This book presents a description of a new theory of memory which departs from existing concepts. It integrates and explains in detail the recent research findings in sentence memory, language comprehension, search on long term memory, verbal learning, and forgetting and imagery. It also reports new experiments which clarify these issues. A major point of integration is the synthesis between verbal learning analysis of human memory and the psycholinguistic analysis of sentences. Among the chapter headings are: (1) "Associationism: A Historical Review," (2) "Computer Simulation Models of Memory," (3) "Current Developments in Linguistics," (4) "The Structure of Knowledge," (5) "The Encoding Problem," (6) "The Recognition Process," (7) "Properties of the Memory Structure," (8) "Fact Retrieval," (9) "Question Answering," (10) "Verbal Learning," and (11) "Interference and Forgetting." (WR)},
	language = {English},
	urldate = {2021-06-21},
	author = {Anderson, John R. and Bower, Gordon H.},
	year = {1973},
	keywords = {Memory, Cognitive Processes, Comprehension, Conceptual Schemes, Language Skills, Linguistics, Models, Recall (Psychology), Retention (Psychology), Verbal Learning},
	file = {Snapshot:/home/paolo/zotero_data/storage/ZASFTYDC/eric.ed.gov.html:text/html},
}

@misc{GordonBowerPsycNET,
	title = {Gordon {H}. {Bower}. - {PsycNET}},
	url = {https://content.apa.org/record/2007-00058-003},
	urldate = {2021-06-21},
	file = {Gordon H. Bower. - PsycNET:/home/paolo/zotero_data/storage/HVMFCKTL/2007-00058-003.html:text/html},
}

@misc{TumultuousHistorySearch,
	title = {The {Tumultuous} {History} of the {Search} for {Artificial} {Intelligence}, {Daniel} {Crevier}. 1993. {Basic} {Books}, {New} {York}, {NY}. 432 pages. {ISBN}: 0-465-02997-3. \$27.50, 1994},
	url = {https://journals.sagepub.com/doi/abs/10.1177/027046769401400414},
	urldate = {2021-06-21},
	file = {The Tumultuous History of the Search for Artificial Intelligence, Daniel Crevier. 1993. Basic Books, New York, NY. 432 pages. ISBN\: 0-465-02997-3. \\\$27.50, 1994:/home/paolo/zotero_data/storage/VEGVXL3K/027046769401400414.html:text/html},
}

@article{colbyComputerMethodPsychotherapy1966,
	title = {A {Computer} {Method} of {Psychotherapy}: {PRELIMINARY} {COMMUNICATION}},
	volume = {142},
	issn = {0022-3018},
	shorttitle = {A {COMPUTER} {METHOD} {OF} {PSYCHOTHERAPY}},
	url = {https://journals.lww.com/jonmd/Citation/1966/02000/A_COMPUTER_METHOD_OF_PSYCHOTHERAPY__PRELIMINARY.5.aspx},
	abstract = {An abstract is unavailable. This article is available as a PDF only.},
	language = {en-US},
	number = {2},
	urldate = {2021-06-21},
	journal = {The Journal of Nervous and Mental Disease},
	author = {Colby, Kenneth Mark and Watt, James B. and Gilbert, John P.},
	month = feb,
	year = {1966},
	pages = {148--152},
	file = {Snapshot:/home/paolo/zotero_data/storage/PVJDRRCY/A_COMPUTER_METHOD_OF_PSYCHOTHERAPY__PRELIMINARY.5.html:text/html},
}

@book{weizenbaumComputerPowerHuman1976,
	title = {Computer power and human reason: {From} judgment to calculation},
	shorttitle = {Computer power and human reason},
	abstract = {Book by Weizenbaum, Joseph},
	author = {Weizenbaum, Joseph},
	month = jan,
	year = {1976},
}

@article{horstComputationalTheoryMind2003,
	title = {The {Computational} {Theory} of {Mind}},
	url = {https://stanford.library.sydney.edu.au/archives/fall2015/entries/computational-mind/},
	urldate = {2021-06-22},
	author = {Horst, Steven},
	month = jul,
	year = {2003},
	file = {Snapshot:/home/paolo/zotero_data/storage/XCA6HXFV/computational-mind.html:text/html},
}

@article{fodorPropositionalAttitudes1978,
	title = {Propositional {Attitudes}},
	volume = {61},
	issn = {0026-9662},
	url = {https://doi.org/10.5840/monist197861444},
	doi = {10.5840/monist197861444},
	number = {4},
	urldate = {2021-06-22},
	journal = {The Monist},
	author = {Fodor, J. A.},
	month = oct,
	year = {1978},
	pages = {501--524},
	file = {Snapshot:/home/paolo/zotero_data/storage/JDBNNU64/1059926.html:text/html},
}

@misc{pressRePresentationsMITPress,
	title = {{RePresentations} {\textbar} {The} {MIT} {Press}},
	url = {https://mitpress.mit.edu/books/representations},
	abstract = {These essays will shape discussion in the philosophy of psychology for years to come. A collection of eleven essays dealing with methodological and empirical issues in cognitive science and in the philosophy of mind, Representations convincingly connects philosophical speculation to concrete empirical research.One of the outstanding methodological issues dealt with is the status of functionalism considered as an alternative to behavioristic and physicalistic accounts. of mental states and properties. The other issue is the status of reductionism considered as an account of the relation between the psychological and physical sciences. The first chapters present the main lines of argument which have made functionalism the currently favored philosophical approach to ontology of the mental.The outlines of a psychology of propositional attitudes which emerges from consideration of current developments in cognitive science are contained in the remaining essays.Not all of these essays are re-presentations. The new introductory essay seeks to present an overview and gives some detailed proposals about the contribution that functionalism makes to the solutions of problems about intentionality. The concluding essay, also not previously published, is a sustained examination of the relation between theories about the structure of concepts and theories about how they are learned. Finally, the essay "Three cheers for propositional attitudes", a critical examination of some of D. C. Dennett's ideas, has been completely rewritten for this volume. A Bradford Book.},
	language = {English},
	urldate = {2021-06-22},
	author = {Press, The MIT},
	file = {Snapshot:/home/paolo/zotero_data/storage/BAM98CPR/representations.html:text/html},
}

@misc{universityWhyComputersCan1984,
	title = {Why {Computers} {Can}'t {Outthink} the {Experts}},
	url = {https://exhibits.stanford.edu/feigenbaum/catalog/nr990gh3548},
	language = {English},
	urldate = {2021-06-23},
	author = {University, © Stanford and {Stanford} and 94305, California},
	year = {1984},
	file = {Snapshot:/home/paolo/zotero_data/storage/8EZTQSNI/nr990gh3548.html:text/html},
}

@book{mccorduckMachinesWhoThink2004,
	title = {Machines {Who} {Think}: {A} {Personal} {Inquiry} into the {History} and {Prospects} of {Artificial} {Intelligence}},
	isbn = {978-1-56881-205-2},
	shorttitle = {Machines {Who} {Think}},
	publisher = {AK Peters Ltd},
	author = {McCorduck, Pamela},
	year = {2004},
}

@article{marrTheoryCerebellarCortex1969,
	title = {A theory of cerebellar cortex},
	volume = {202},
	issn = {1469-7793},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1969.sp008820},
	doi = {10.1113/jphysiol.1969.sp008820},
	abstract = {1. A detailed theory of cerebellar cortex is proposed whose consequence is that the cerebellum learns to perform motor skills. Two forms of input—output relation are described, both consistent with the cortical theory. One is suitable for learning movements (actions), and the other for learning to maintain posture and balance (maintenance reflexes). 2. It is known that the cells of the inferior olive and the cerebellar Purkinje cells have a special one-to-one relationship induced by the climbing fibre input. For learning actions, it is assumed that: (a) each olivary cell responds to a cerebral instruction for an elemental movement. Any action has a defining representation in terms of elemental movements, and this representation has a neural expression as a sequence of firing patterns in the inferior olive; and (b) in the correct state of the nervous system, a Purkinje cell can initiate the elemental movement to which its corresponding olivary cell responds. 3. Whenever an olivary cell fires, it sends an impulse (via the climbing fibre input) to its corresponding Purkinje cell. This Purkinje cell is also exposed (via the mossy fibre input) to information about the context in which its olivary cell fired; and it is shown how, during rehearsal of an action, each Purkinje cell can learn to recognize such contexts. Later, when the action has been learnt, occurrence of the context alone is enough to fire the Purkinje cell, which then causes the next elemental movement. The action thus progresses as it did during rehearsal. 4. It is shown that an interpretation of cerebellar cortex as a structure which allows each Purkinje cell to learn a number of contexts is consistent both with the distributions of the various types of cell, and with their known excitatory or inhibitory natures. It is demonstrated that the mossy fibre-granule cell arrangement provides the required pattern discrimination capability. 5. The following predictions are made. (a) The synapses from parallel fibres to Purkinje cells are facilitated by the conjunction of presynaptic and climbing fibre (or post-synaptic) activity. (b) No other cerebellar synapses are modifiable. (c) Golgi cells are driven by the greater of the inputs from their upper and lower dendritic fields. 6. For learning maintenance reflexes, 2(a) and 2(b) are replaced by 2′. Each olivary cell is stimulated by one or more receptors, all of whose activities are usually reduced by the results of stimulating the corresponding Purkinje cell. 7. It is shown that if (2′) is satisfied, the circuit receptor → olivary cell → Purkinje cell → effector may be regarded as a stabilizing reflex circuit which is activated by learned mossy fibre inputs. This type of reflex has been called a learned conditional reflex, and it is shown how such reflexes can solve problems of maintaining posture and balance. 8. 5(a), and either (2) or (2′) are essential to the theory: 5(b) and 5(c) are not absolutely essential, and parts of the theory could survive the disproof of either.},
	language = {English},
	number = {2},
	urldate = {2021-06-23},
	journal = {The Journal of Physiology},
	author = {Marr, David},
	year = {1969},
	note = {tex.copyright: © 1969 The Physiological Society},
	pages = {437--470},
	file = {Marr-1969-A_theory_of_cerebellar_cortex.pdf:/home/paolo/zotero_data/storage/I9MMM873/Marr-1969-A_theory_of_cerebellar_cortex.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/J5AQZWW3/jphysiol.1969.html:text/html},
}

@article{marrTheoryCerebralNeocortex1970,
	title = {A theory for cerebral neocortex},
	volume = {176},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.1970.0040},
	doi = {10.1098/rspb.1970.0040},
	abstract = {It is proposed that the learning of many tasks by the cerebrum is based on using a very few fundamental techniques for organizing information. It is argued that this is made possible by the prevalence in the world of a particular kind of redundancy, which is characterized by a ‘Fundamental Hypothesis’. This hypothesisis used to found a theory of the basic operations which, it is proposed, are carried out by the cerebral neocortex. They involve the use of past experience to form so-called ‘classificatory units’ with which to interpret subsequent experience. Such classificatory units are imagined to be created whenever either something occurs frequently in the brain’s experience, or enough redundancy appears in the form of clusters of slightly differing inputs. A(non-Bayesian) information theoretic account is given of the diagnosis of an input as an instance of an existing classificatory unit, and of the interpretation as such of an incompletely specified input. Neural models are devised to implement the two operations of diagnosis and interpretation, and it is found that the performance of the second is an automatic consequence of the model’s ability to perform the first. The discovery and formation of new classificatory units is discussed within the context of these neural models. It is shown how a climbing fibre input (of the kind described by Cajal) to the correct cell can cause that cell to perform a mountain-climbing operation in an underlying probability space, that will lead it to respond to a class of events for which it is appropriate to code. This is called the ‘spatial recognizer effect’. The structure of the cerebral neocortex is reviewed in the light of the model which the theory establishes. It is found that many elements in the cortex have a natural identification with elements in the model. This enables many predictions, with specified degrees of firmness, to be made concerning the connexions and synapses of the following cortical cells and fibres: Martinotti cells; cerebral granule cells; pyramidal cells of layers III, V and II; short axon cells of all layers, especially I, IV and VI; cerebral climbing fibres and those cells of the cortex which give rise to them; cerebral basket cells; fusiform cells of layers VI and VII. It is shown that if rather little information about the classificatory units to be formed has been coded genetically, it may be necessary to use a technique called codon formation to organize structure in a suitable way to represent a new unit. It is shown that under certain conditions, it is necessary to carry out a part of this organization during sleep. A prediction is made about the effect of sleep on learning of a certain kind.},
	number = {1043},
	urldate = {2021-06-23},
	journal = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
	author = {Marr, D. and Brindley, Giles Skey},
	month = nov,
	year = {1970},
	pages = {161--234},
	file = {Snapshot:/home/paolo/zotero_data/storage/C89TAYBW/rspb.1970.html:text/html},
}

@article{marrSimpleMemoryTheory1971,
	title = {Simple memory: a theory for archicortex},
	volume = {262},
	shorttitle = {Simple memory},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.1971.0078},
	doi = {10.1098/rstb.1971.0078},
	abstract = {It is proposed that the most important characteristic of archicortex is its ability to perform a simple kind of memorizing task. It is shown that rather general numerical constraints roughly determine the dimensions of memorizing models for the mammalian brain, and from these is derived a general model for archicortex. The addition of further constraints leads to the notion of a simple representation, which is a way of translating a great deal of information into the firing of about 200 out of a population of 105 cells. It is shown that if about 105 simple representations are stored in such a population of cells, very little information about a single learnt event is necessary to provoke its recall. A detailed numerical examination is made of a particular example of this kind of memory, and various general conclusions are drawn from the analysis. The insight gained from these models is used to derive theories for various archicortical areas. A functional interpretation is given of the cells and synapses of the area entorhinalis, the presubiculum, the prosubiculum, the cornu ammonis and the fascia dentata. Many predictions are made, a substantial number of which must be true if the theory is correct. A general functional classification of typical archicortical cells is proposed.},
	number = {841},
	urldate = {2021-06-23},
	journal = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
	author = {Marr, D. and Brindley, Giles Skey},
	month = jul,
	year = {1971},
	pages = {23--81},
	file = {Snapshot:/home/paolo/zotero_data/storage/TVPNG2WN/rstb.1971.html:text/html},
}

@article{marrUnderstandingComputationUnderstanding1976,
	title = {From {Understanding} {Computation} to {Understanding} {Neural} {Circuitry}},
	url = {https://dspace.mit.edu/handle/1721.1/5782},
	abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
	language = {English},
	urldate = {2021-06-23},
	author = {Marr, D. and Poggio, T.},
	month = may,
	year = {1976},
	file = {Marr_Poggio-1976-From_Understanding_Computation_to_Understanding_Neural_Circuitry.pdf:/home/paolo/zotero_data/storage/UWKM4IDH/Marr_Poggio-1976-From_Understanding_Computation_to_Understanding_Neural_Circuitry.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/KXPBBBN7/5782.html:text/html},
}

@article{brooksElephantsDonPlay1990,
	title = {Elephants {Don}'t {Play} {Chess}},
	language = {English},
	author = {Brooks, Rodney A},
	year = {1990},
	pages = {12},
	file = {Brooks-1990-Elephants_Don't_Play_Chess.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1990/Brooks-1990-Elephants_Don't_Play_Chess.pdf:application/pdf},
}

@misc{minskySocietyMind1986,
	title = {The {Society} of {Mind}},
	url = {https://www.amazon.com/Society-Mind-Marvin-Minsky/dp/0671657135},
	urldate = {2021-06-24},
	author = {Minsky, Marvin},
	year = {1986},
	file = {The Society of Mind\: Minsky, Marvin\: 9780671657130\: Amazon.com\: Books:/home/paolo/zotero_data/storage/H44VLIA3/0671657135.html:text/html},
}

@book{moravecMindChildrenFuture1988,
	title = {Mind children: {The} future of robot and human intelligence},
	shorttitle = {Mind children},
	publisher = {Harvard University Press},
	author = {Moravec, Hans},
	year = {1988},
	file = {Snapshot:/home/paolo/zotero_data/storage/CB574ZB5/books.html:text/html},
}

@book{fodorModularityMind1983,
	title = {The modularity of mind},
	publisher = {MIT press},
	author = {Fodor, Jerry A.},
	year = {1983},
	file = {Fodor-1983-The_modularity_of_mind.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1983/Fodor-1983-The_modularity_of_mind.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/6EWX73X2/books.html:text/html},
}

@book{pearlProbabilisticReasoningIntelligent1988,
	address = {San Francisco, CA, USA},
	title = {Probabilistic {Reasoning} in {Intelligent} {Systems}: {Networks} of {Plausible} {Inference}},
	isbn = {978-1-55860-479-7},
	shorttitle = {Probabilistic {Reasoning} in {Intelligent} {Systems}},
	abstract = {From the Publisher: Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertaintyand offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognitionin short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Pearl, Judea},
	year = {1988},
	file = {Pearl-1988-Probabilistic_Reasoning_in_Intelligent_Systems.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1988/Pearl-1988-Probabilistic_Reasoning_in_Intelligent_Systems.pdf:application/pdf},
}

@misc{PETScanPET2012,
	title = {{PET} {Scan}: {PET}/{CT} {History}},
	shorttitle = {{PET} {Scan}},
	url = {https://web.archive.org/web/20120414014653/http://www.petscaninfo.com/zportal/portals/phys/petct/history},
	urldate = {2021-06-24},
	month = apr,
	year = {2012},
	file = {Snapshot:/home/paolo/zotero_data/storage/EWANA7FB/history.html:text/html},
}

@misc{pressSoarCognitiveArchitecture2012,
	title = {The {Soar} {Cognitive} {Architecture} {\textbar} {The} {MIT} {Press}},
	url = {https://mitpress.mit.edu/books/soar-cognitive-architecture},
	abstract = {The definitive presentation of Soar, one AI's most enduring architectures, offering comprehensive descriptions of fundamental aspects and new components. 
                In development for thirty years, Soar is a general cognitive architecture that integrates knowledge-intensive reasoning, reactive execution, hierarchical reasoning, planning, and learning from experience, with the goal of creating a general computational system that has the same cognitive abilities as humans. In contrast, most AI systems are designed to solve only one type of problem, such as playing chess, searching the Internet, or scheduling aircraft departures. Soar is both a software system for agent development and a theory of what computational structures are necessary to support human-level agents. Over the years, both software system and theory have evolved. This book offers the definitive presentation of Soar from theoretical and practical perspectives, providing comprehensive descriptions of fundamental aspects and new components.The current version of Soar features major extensions, adding reinforcement learning, semantic memory, episodic memory, mental imagery, and an appraisal-based model of emotion. This book describes details of Soar's component memories and processes and offers demonstrations of individual components, components working in combination, and real-world applications. Beyond these functional considerations, the book also proposes requirements for general cognitive architectures and explicitly evaluates how well Soar meets those requirements.},
	language = {en},
	urldate = {2021-06-24},
	author = {Press, The MIT},
	year = {2012},
	note = {Publisher: The MIT Press},
	file = {Snapshot:/home/paolo/zotero_data/storage/YD8YDYM3/soar-cognitive-architecture.html:text/html},
}

@inproceedings{bankoScalingVeryVery2001,
	title = {Scaling to very very large corpora for natural language disambiguation},
	booktitle = {Proceedings of the 39th annual meeting of the {Association} for {Computational} {Linguistics}},
	author = {Banko, Michele and Brill, Eric},
	year = {2001},
	pages = {26--33},
	file = {Banko_Brill-2001-Scaling_to_very_very_large_corpora_for_natural_language_disambiguation.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2001/Banko_Brill-2001-Scaling_to_very_very_large_corpora_for_natural_language_disambiguation.pdf:application/pdf},
}

@article{halevyUnreasonableEffectivenessData2009,
	title = {The {Unreasonable} {Effectiveness} of {Data}},
	volume = {24},
	issn = {1541-1672},
	url = {http://ieeexplore.ieee.org/document/4804817/},
	doi = {10.1109/MIS.2009.36},
	language = {en},
	number = {2},
	urldate = {2021-06-24},
	journal = {IEEE Intelligent Systems},
	author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
	month = mar,
	year = {2009},
	pages = {8--12},
	file = {Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf:/home/paolo/zotero_data/storage/3DHTX48T/Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf:application/pdf},
}

@article{thulbornRoleFerritinHemosiderin1990,
	title = {The role of ferritin and hemosiderin in the {MR} appearance of cerebral hemorrhage: a histopathologic biochemical study in rats.},
	volume = {154},
	shorttitle = {The role of ferritin and hemosiderin in the {MR} appearance of cerebral hemorrhage},
	number = {5},
	journal = {AJR. American journal of roentgenology},
	author = {Thulborn, Keith R. and Sorensen, A. Gregory and Kowall, N. W. and Mckee, Ann and Lai, Albert and Mckinstry, Robert C. and Moore, J. and Rosen, B. R. and Brady, T. J.},
	year = {1990},
	note = {Publisher: Am Roentgen Ray Soc},
	pages = {1053--1059},
	file = {Thulborn_et_al-1990-The_role_of_ferritin_and_hemosiderin_in_the_MR_appearance_of_cerebral_hemorrhage.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1990/Thulborn_et_al-1990-The_role_of_ferritin_and_hemosiderin_in_the_MR_appearance_of_cerebral_hemorrhage.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/VSMYLEML/ajr.154.5.html:text/html},
}

@article{ogawaBrainMagneticResonance1990,
	title = {Brain magnetic resonance imaging with contrast dependent on blood oxygenation},
	volume = {87},
	number = {24},
	journal = {proceedings of the National Academy of Sciences},
	author = {Ogawa, Seiji and Lee, Tso-Ming and Kay, Alan R. and Tank, David W.},
	year = {1990},
	note = {Publisher: National Acad Sciences},
	pages = {9868--9872},
	file = {Ogawa_et_al-1990-Brain_magnetic_resonance_imaging_with_contrast_dependent_on_blood_oxygenation.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1990/Ogawa_et_al-1990-Brain_magnetic_resonance_imaging_with_contrast_dependent_on_blood_oxygenation.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/M8ZCDA4Q/9868.html:text/html},
}

@article{kwongDynamicMagneticResonance1992,
	title = {Dynamic magnetic resonance imaging of human brain activity during primary sensory stimulation.},
	volume = {89},
	number = {12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kwong, Kenneth K. and Belliveau, John W. and Chesler, David A. and Goldberg, Inna E. and Weisskoff, Robert M. and Poncelet, Brigitte P. and Kennedy, David N. and Hoppel, Bernice E. and Cohen, Mark S. and Turner, Robert},
	year = {1992},
	note = {Publisher: National Acad Sciences},
	pages = {5675--5679},
	file = {Kwong_et_al-1992-Dynamic_magnetic_resonance_imaging_of_human_brain_activity_during_primary.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1992/Kwong_et_al-1992-Dynamic_magnetic_resonance_imaging_of_human_brain_activity_during_primary.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/GG7V5FAP/5675.html:text/html},
}

@article{horvathTranscranialMagneticStimulation2011,
	title = {Transcranial magnetic stimulation: a historical evaluation and future prognosis of therapeutically relevant ethical concerns},
	volume = {37},
	copyright = {© 2011, Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.},
	issn = {0306-6800, 1473-4257},
	shorttitle = {Transcranial magnetic stimulation},
	url = {https://jme.bmj.com/content/37/3/137},
	doi = {10.1136/jme.2010.039966},
	abstract = {{\textless}p{\textgreater}Transcranial Magnetic Stimulation (TMS) is a non-invasive neurostimulatory and neuromodulatory technique increasingly used in clinical and research practices around the world. Historically, the ethical considerations guiding the therapeutic practice of TMS were largely concerned with aspects of subject safety in clinical trials. While safety remains of paramount importance, the recent US Food and Drug Administration approval of the Neuronetics NeuroStar TMS device for the treatment of specific medication-resistant depression has raised a number of additional ethical concerns, including marketing, off-label use and technician certification. This article provides an overview of the history of TMS and highlights the ethical questions that are likely arise as the therapeutic use of TMS continues to expand.{\textless}/p{\textgreater}},
	language = {en},
	number = {3},
	urldate = {2021-06-25},
	journal = {Journal of Medical Ethics},
	author = {Horvath, Jared C. and Perez, Jennifer M. and Forrow, Lachlan and Fregni, Felipe and Pascual-Leone, Alvaro},
	month = mar,
	year = {2011},
	pmid = {21106996},
	note = {Publisher: Institute of Medical Ethics
Section: Clinical ethics},
	pages = {137--143},
}

@article{ferrariBriefReviewHistory2012,
	title = {A brief review on the history of human functional near-infrared spectroscopy ({fNIRS}) development and fields of application},
	volume = {63},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811912003308},
	doi = {10.1016/j.neuroimage.2012.03.049},
	abstract = {This review is aimed at celebrating the upcoming 20th anniversary of the birth of human functional near-infrared spectroscopy (fNIRS). After the discovery in 1992 that the functional activation of the human cerebral cortex (due to oxygenation and hemodynamic changes) can be explored by NIRS, human functional brain mapping research has gained a new dimension. fNIRS or optical topography, or near-infrared imaging or diffuse optical imaging is used mainly to detect simultaneous changes in optical properties of the human cortex from multiple measurement sites and displays the results in the form of a map or image over a specific area. In order to place current fNIRS research in its proper context, this paper presents a brief historical overview of the events that have shaped the present status of fNIRS. In particular, technological progresses of fNIRS are highlighted (i.e. from single-site to multi-site functional cortical measurements (images)), introduction of the commercial multi-channel systems, recent commercial wireless instrumentation and more advanced prototypes.},
	language = {en},
	number = {2},
	urldate = {2021-06-25},
	journal = {NeuroImage},
	author = {Ferrari, Marco and Quaresima, Valentina},
	month = nov,
	year = {2012},
	keywords = {Cortical activation, Functional near-infrared spectroscopy, Functional near-infrared topography, Hemodynamic response, Optical imaging},
	pages = {921--935},
	file = {ScienceDirect Snapshot:/home/paolo/zotero_data/storage/6HC7C7HF/S1053811912003308.html:text/html},
}

@book{fodorElmExpertMentalese1994,
	title = {The {Elm} and the {Expert}: {Mentalese} and {Its} {Semantics}},
	isbn = {978-0-262-56093-1},
	shorttitle = {The {Elm} and the {Expert}},
	abstract = {Written in a highly readable, irreverent style, The Elm and the Expert provides a lively discussion of semantic issues about mental representation, with special attention to issues raised by Frege's problem, Twin cases, and the putative indeterminacy of reference.Bound to be widely read and much discussed, The Elm and the Expert, written in Jerry Fodor's usual highly readable, irreverent style, provides a lively discussion of semantic issues about mental representation, with special attention to issues raised by Frege's problem, Twin cases, and the putative indeterminacy of reference. The book extends and revises a view of the relation between mind and meaning that the author has been developing since his 1975 book, The Language of Thought.There is a general consensus among philosophers that a referential semantics for mental representation cannot support a robust account of intentional explanation. Fodor has himself espoused this view in previous publications, and it is widespread (if tacit) throughout the cognitive science community. This book is largely a reconsideration of the arguments that are supposed to ground this consensus. Fodor concludes that these considerations are far less decisive than has been supposed. He offers a theory sketch in which psychological explanation is intentional, psychological processes are computational, and the semantic properties of mental representations are referential. Connections with the problem of naturalizing intentionality are also explored.The four lectures in The Elm and the Expert were originally delivered in Paris in the spring of 1993 to inaugurate the Jean Nicod Lecture series. The Jean Nicod Lectures are delivered annually by a leading philosopher of mind or philosophically oriented cognitive scientist. The 1993 lectures marked the centenary of the birth of the French philosopher and logician Jean Nicod (1893-1931). The lectures are sponsored by the Centre National de la Recherche Scientifique (CNRS) as part of its effort to develop the interdisciplinary field of cognitive science in France.Jean Nicod series},
	language = {en},
	publisher = {MIT Press},
	author = {Fodor, Jerry A.},
	year = {1994},
	note = {Google-Books-ID: JIUCnxEZTXkC},
	keywords = {Psychology / Cognitive Psychology \& Cognition},
}

@article{clarkExtendedMind1998b,
	title = {The {Extended} {Mind}},
	volume = {58},
	issn = {0003-2638},
	url = {https://doi.org/10.1093/analys/58.1.7},
	doi = {10.1093/analys/58.1.7},
	number = {1},
	urldate = {2021-06-25},
	journal = {Analysis},
	author = {Clark, Andy and Chalmers, David},
	month = jan,
	year = {1998},
	pages = {7--19},
	file = {Snapshot:/home/paolo/zotero_data/storage/5P8IQ3KA/153111.html:text/html},
}

@book{clarkNaturalbornCyborgsMinds2003,
	title = {Natural-born cyborgs: {Minds}, technologies, and the future of human intelligence},
	shorttitle = {Natural-born cyborgs},
	publisher = {Oxford University Press},
	author = {Clark, Andy},
	year = {2003},
	file = {Clark-2003-Natural-born_cyborgs.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2003/Clark-2003-Natural-born_cyborgs.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/ZK357LE2/books.html:text/html},
}

@article{fristonFreeEnergyPrinciple2006,
	series = {Theoretical and {Computational} {Neuroscience}: {Understanding} {Brain} {Functions}},
	title = {A free energy principle for the brain},
	volume = {100},
	issn = {0928-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S092842570600060X},
	doi = {10.1016/j.jphysparis.2006.10.001},
	abstract = {By formulating Helmholtz’s ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses. In this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy principle. The free energy considered here measures the difference between the probability distribution of environmental quantities that act on the system and an arbitrary distribution encoded by its configuration. The system can minimise free energy by changing its configuration to affect the way it samples the environment or change the distribution it encodes. These changes correspond to action and perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment assumes that the system’s state and structure encode an implicit and probabilistic model of the environment. We will look at the models entailed by the brain and how minimisation of its free energy can explain its dynamics and structure.},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {Journal of Physiology-Paris},
	author = {Friston, Karl and Kilner, James and Harrison, Lee},
	month = jul,
	year = {2006},
	keywords = {Action, Attention, Free energy, Hierarchical, Inference, Learning, Perception, Selection, Variational Bayes},
	pages = {70--87},
	file = {Friston_et_al-2006-A_free_energy_principle_for_the_brain.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2006/Friston_et_al-2006-A_free_energy_principle_for_the_brain.pdf:application/pdf;ScienceDirect Snapshot:/home/paolo/zotero_data/storage/E6H54Q96/S092842570600060X.html:text/html},
}

@article{tononiInformationIntegrationTheory2004,
	title = {An information integration theory of consciousness},
	volume = {5},
	issn = {1471-2202},
	url = {https://doi.org/10.1186/1471-2202-5-42},
	doi = {10.1186/1471-2202-5-42},
	abstract = {Consciousness poses two main problems. The first is understanding the conditions that determine to what extent a system has conscious experience. For instance, why is our consciousness generated by certain parts of our brain, such as the thalamocortical system, and not by other parts, such as the cerebellum? And why are we conscious during wakefulness and much less so during dreamless sleep? The second problem is understanding the conditions that determine what kind of consciousness a system has. For example, why do specific parts of the brain contribute specific qualities to our conscious experience, such as vision and audition?},
	number = {1},
	urldate = {2021-06-25},
	journal = {BMC Neuroscience},
	author = {Tononi, Giulio},
	month = nov,
	year = {2004},
	keywords = {Binocular Rivalry, Conscious Experience, Conscious State, Firing Pattern, Information Integration},
	pages = {42},
	file = {Tononi-2004-An_information_integration_theory_of_consciousness.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2004/Tononi-2004-An_information_integration_theory_of_consciousness.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/5UQU6W54/1471-2202-5-42.html:text/html},
}

@book{mccarthyProgramsCommonSense1960,
	title = {Programs with common sense},
	publisher = {RLE and MIT computation center},
	author = {McCarthy, John},
	year = {1960},
	file = {McCarthy-1960-Programs_with_common_sense.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1960/McCarthy-1960-Programs_with_common_sense.pdf:application/pdf},
}

@article{lecunBackpropagationAppliedHandwritten1989,
	title = {Backpropagation applied to handwritten zip code recognition},
	volume = {1},
	number = {4},
	journal = {Neural computation},
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
	year = {1989},
	note = {Publisher: MIT Press},
	pages = {541--551},
	file = {LeCun_et_al-1989-Backpropagation_applied_to_handwritten_zip_code_recognition.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1989/LeCun_et_al-1989-Backpropagation_applied_to_handwritten_zip_code_recognition.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/8WWPRJZA/Backpropagation-Applied-to-Handwritten-Zip-Code.html:text/html},
}

@inproceedings{wengCresceptronSelforganizingNeural1992,
	title = {Cresceptron: a self-organizing neural network which grows adaptively},
	shorttitle = {Cresceptron},
	abstract = {Cresceptron represents a new approach t o neural networks. It uses a hierarchical f m m e w o r k t o grow neural networks automatically, adaptively and incrementally through learning. A t every level of the hierarchy, new concepts are detected automatically and the network grows by creating new neurons and synapses which memorize the new concepts and their context. The training samples are generalized t o other perceptually equivalent i t ems through hierarchical tolerance of deviation. The neural network recognizes the learned i tems and their variations by hierarchically associating the learned knowledge with the input. I t segments the recognized i tems f r o m the input through back tracking along the response paths. 1},
	booktitle = {In {Proc}. {Int}’l {Joint} {Conference} on {Neural} {Networks}},
	author = {Weng, John (juyang and Ahuja, Narendra and Huang, Thomas S.},
	year = {1992},
	pages = {576--581},
	file = {Weng_et_al-1992-Cresceptron.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1992/Weng_et_al-1992-Cresceptron.pdf:application/pdf;Citeseer - Snapshot:/home/paolo/zotero_data/storage/TKKX7DC8/summary.html:text/html},
}

@article{decarvalhoIntegratedBooleanNeural1994,
	title = {An integrated {Boolean} neural network for pattern classification},
	volume = {15},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/0167865594900094},
	doi = {10.1016/0167-8655(94)90009-4},
	abstract = {This paper describes an integrated approach to pattern classification where a self-organising Boolean neural network architecture is used as a front-end processor to a feedforward neural architecture based on goal-seeking principles (the GSN architecture). The performance of the integrated architecture is illustrated by considering its application to a character recognition problem.},
	language = {en},
	number = {8},
	urldate = {2021-06-27},
	journal = {Pattern Recognition Letters},
	author = {de Carvalho, A. and Fairhurst, M. C. and Bisset, D. L.},
	month = aug,
	year = {1994},
	keywords = {Boolean neural networks, Integrated architectures, Pattern recognition},
	pages = {807--813},
	file = {ScienceDirect Snapshot:/home/paolo/zotero_data/storage/99X2N2SA/0167865594900094.html:text/html},
}

@article{hintonWakesleepAlgorithmUnsupervised1995,
	title = {The "wake-sleep" algorithm for unsupervised neural networks},
	volume = {268},
	copyright = {© 1995},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/268/5214/1158},
	doi = {10.1126/science.7761831},
	abstract = {{\textless}p{\textgreater}An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up "recognition" connections convert the input into representations in successive hidden layers, and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above. In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the "sleep" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.{\textless}/p{\textgreater}},
	language = {en},
	number = {5214},
	urldate = {2021-06-27},
	journal = {Science},
	author = {Hinton, G. E. and Dayan, P. and Frey, B. J. and Neal, R. M.},
	month = may,
	year = {1995},
	pmid = {7761831},
	note = {Publisher: American Association for the Advancement of Science
Section: Reports},
	pages = {1158--1161},
	file = {Snapshot:/home/paolo/zotero_data/storage/IYEV8Z4J/1158.html:text/html},
}

@incollection{behnkeNeuralAbstractionPyramid2003,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Abstraction} {Pyramid} {Architecture}},
	isbn = {978-3-540-45169-3},
	url = {https://doi.org/10.1007/978-3-540-45169-3_4},
	abstract = {The last two chapters reviewed what is known about object recognition in the human brain and how the concepts of hierarchy and recurrence have been applied to image processing. Now it is time to put both together.In this chapter, an architecture for image interpretation is defined that will be used for the remainder of this thesis. I will refer to this architecture as the Neural Abstraction Pyramid. The Neural Abstraction Pyramid is a neurobiologically inspired hierarchical neural network with local recurrent connectivity. Images are represented at multiple levels of abstraction. Local connections form horizontal and vertical feedback loops between simple processing elements. This allows to resolve ambiguities by the flexible use of partial interpretation results as context.},
	language = {en},
	urldate = {2021-06-27},
	booktitle = {Hierarchical {Neural} {Networks} for {Image} {Interpretation}},
	publisher = {Springer},
	author = {Behnke, Sven},
	editor = {Behnke, Sven},
	year = {2003},
	doi = {10.1007/978-3-540-45169-3_4},
	keywords = {Feature Cell, Human Visual System, Output Unit, Processing Element, Transfer Function},
	pages = {65--94},
}

@article{hochreiterLongShortTermMemory1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2021-06-27},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Hochreiter_Schmidhuber-1997-Long_Short-Term_Memory.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1997/Hochreiter_Schmidhuber-1997-Long_Short-Term_Memory.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/TNXCHRZ2/Long-Short-Term-Memory.html:text/html},
}

@article{galesApplicationHiddenMarkov2007,
	title = {The {Application} of {Hidden} {Markov} {Models} in {Speech} {Recognition}},
	volume = {1},
	issn = {1932-8346, 1932-8354},
	url = {http://www.nowpublishers.com/article/Details/SIG-004},
	doi = {10.1561/2000000004},
	abstract = {Hidden Markov Models (HMMs) provide a simple and eﬀective framework for modelling time-varying spectral vector sequences. As a consequence, almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs.},
	language = {en},
	number = {3},
	urldate = {2021-06-27},
	journal = {Foundations and Trends® in Signal Processing},
	author = {Gales, Mark and Young, Steve},
	year = {2007},
	pages = {195--304},
	file = {Gales and Young - 2007 - The Application of Hidden Markov Models in Speech .pdf:/home/paolo/zotero_data/storage/7FJ29PEX/Gales and Young - 2007 - The Application of Hidden Markov Models in Speech .pdf:application/pdf},
}

@article{lecunDeepLearning2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2021-06-27},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/home/paolo/zotero_data/storage/VHLXQ59I/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@misc{MachineLearning2019,
	title = {Machine {Learning}},
	url = {https://deepai.org/machine-learning-glossary-and-terms/machine-learning},
	abstract = {A field of computer science that aims to teach computers how to learn and act without being explicitly programmed.},
	urldate = {2021-06-28},
	journal = {DeepAI},
	month = may,
	year = {2019},
}

@article{zhangStudyArtificialIntelligence2021,
	title = {Study on artificial intelligence: {The} state of the art and future prospects},
	volume = {23},
	issn = {2452-414X},
	shorttitle = {Study on artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000248},
	doi = {10.1016/j.jii.2021.100224},
	abstract = {In the world, the technological and industrial revolution is accelerating by the widespread application of new generation information and communication technologies, such as AI, IoT (the Internet of Things), and blockchain technology. Artificial intelligence has attracted much attention from government, industry, and academia. In this study, popular articles published in recent years that relate to artificial intelligence are selected and explored. This study aims to provide a review of artificial intelligence based on industry information integration. It presents an overview of the scope of artificial intelligence using background, drivers, technologies, and applications, as well as logical opinions regarding the development of artificial intelligence. This paper may play a role in AI-related research and should provide important insights for practitioners in the real world.The main contribution of this study is that it clarifies the state of the art of AI for future study.},
	language = {en},
	urldate = {2021-06-28},
	journal = {Journal of Industrial Information Integration},
	author = {Zhang, Caiming and Lu, Yang},
	month = sep,
	year = {2021},
	keywords = {Artificial intelligence (AI), Machine learning, Natural language processing (NLP)},
	pages = {100224},
	file = {Zhang_Lu-2021-Study_on_artificial_intelligence.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2021/Zhang_Lu-2021-Study_on_artificial_intelligence.pdf:application/pdf;ScienceDirect Snapshot:/home/paolo/zotero_data/storage/6BBDQ3AH/S2452414X21000248.html:text/html},
}

@misc{parloffWhyDeepLearning2016,
	title = {Why {Deep} {Learning} {Is} {Suddenly} {Changing} {Your} {Life}},
	url = {https://fortune.com/longform/ai-artificial-intelligence-deep-machine-learning/},
	abstract = {And will soon transform corporate America.},
	language = {en},
	urldate = {2021-06-28},
	journal = {Fortune},
	author = {Parloff, Roger},
	year = {2016},
}

@article{ciresanMulticolumnDeepNeural2012,
	title = {Multi-column {Deep} {Neural} {Networks} for {Image} {Classification}},
	url = {http://arxiv.org/abs/1202.2745},
	abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	urldate = {2021-06-28},
	journal = {arXiv:1202.2745 [cs]},
	author = {Cireşan, Dan and Meier, Ueli and Schmidhuber, Juergen},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.2745},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Cireşan_et_al-2012-Multi-column_Deep_Neural_Networks_for_Image_Classification.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2012/Cireşan_et_al-2012-Multi-column_Deep_Neural_Networks_for_Image_Classification.pdf:application/pdf;arXiv.org Snapshot:/home/paolo/zotero_data/storage/HSVV9AIE/1202.html:text/html},
}

@article{tanEfficientNetV2SmallerModels2021,
	title = {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},
	shorttitle = {{EfficientNetV2}},
	url = {http://arxiv.org/abs/2104.00298},
	abstract = {This paper introduces EfﬁcientNetV2, a new family of convolutional networks that have faster training speed and better parameter efﬁciency than previous models. To develop these models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efﬁciency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfﬁcientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.},
	language = {en},
	urldate = {2021-06-29},
	journal = {arXiv:2104.00298 [cs]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.00298},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Tan and Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf:/home/paolo/zotero_data/storage/8R9J3DAZ/Tan and Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf:application/pdf},
}

@article{brockHighPerformanceLargeScaleImage2021,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	url = {http://arxiv.org/abs/2102.06171},
	abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
	urldate = {2021-06-29},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.06171},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Brock_et_al-2021-High-Performance_Large-Scale_Image_Recognition_Without_Normalization.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2021/Brock_et_al-2021-High-Performance_Large-Scale_Image_Recognition_Without_Normalization.pdf:application/pdf;arXiv.org Snapshot:/home/paolo/zotero_data/storage/FN6X4B5Q/2102.html:text/html},
}

@article{ronnebergerUNetConvolutionalNetworks2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-06-29},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ronneberger_et_al-2015-U-Net.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2015/Ronneberger_et_al-2015-U-Net.pdf:application/pdf;arXiv.org Snapshot:/home/paolo/zotero_data/storage/QRNFQQE4/1505.html:text/html},
}

@inproceedings{bahetiEffUNetNovelArchitecture2020,
	address = {Seattle, WA, USA},
	title = {Eff-{UNet}: {A} {Novel} {Architecture} for {Semantic} {Segmentation} in {Unstructured} {Environment}},
	isbn = {978-1-72819-360-1},
	shorttitle = {Eff-{UNet}},
	url = {https://ieeexplore.ieee.org/document/9150621/},
	doi = {10.1109/CVPRW50498.2020.00187},
	abstract = {Since the last few decades, the number of road causalities has seen continuous growth across the globe. Nowadays intelligent transportation systems are being developed to enable safe and relaxed driving and scene understanding of the surrounding environment is an integral part of it. While several approaches are being developed for semantic scene segmentation based on deep learning and Convolutional Neural Network (CNN), these approaches assume well structured road infrastructure and driving environment. We focus our work on recent India Driving Lite Dataset (IDD), which contains data from unstructured driving environment and was hosted as an online challenge in NCVPRIPG 2019. We propose a novel architecture named as Eff-UNet which combines the effectiveness of compound scaled EfﬁcientNet as the encoder for feature extraction with UNet decoder for reconstructing the ﬁne-grained segmentation map. High level feature information as well as low level spatial information useful for precise segmentation are combined. The proposed architecture achieved 0.7376 and 0.6276 mean Intersection over Union (mIoU) on validation and test dataset respectively and won ﬁrst prize in IDD lite segmentation challenge outperforming other approaches in the literature.},
	language = {en},
	urldate = {2021-06-29},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Baheti, Bhakti and Innani, Shubham and Gajre, Suhas and Talbar, Sanjay},
	month = jun,
	year = {2020},
	pages = {1473--1481},
	file = {Baheti et al. - 2020 - Eff-UNet A Novel Architecture for Semantic Segmen.pdf:/home/paolo/zotero_data/storage/Q2G7CS9F/Baheti et al. - 2020 - Eff-UNet A Novel Architecture for Semantic Segmen.pdf:application/pdf},
}

@article{zhangVarifocalNetIoUawareDense2020,
	title = {{VarifocalNet}: {An} {IoU}-aware {Dense} {Object} {Detector}},
	shorttitle = {{VarifocalNet}},
	url = {https://arxiv.org/abs/2008.13367v2},
	abstract = {Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. Prior work uses the classification score or a combination of classification and predicted localization scores to rank candidates. However, neither option results in a reliable ranking, thus degrading detection performance. In this paper, we propose to learn an Iou-aware Classification Score (IACS) as a joint representation of object presence confidence and localization accuracy. We show that dense object detectors can achieve a more accurate ranking of candidate detections based on the IACS. We design a new loss function, named Varifocal Loss, to train a dense object detector to predict the IACS, and propose a new star-shaped bounding box feature representation for IACS prediction and bounding box refinement. Combining these two new components and a bounding box refinement branch, we build an IoU-aware dense object detector based on the FCOS+ATSS architecture, that we call VarifocalNet or VFNet for short. Extensive experiments on MS COCO show that our VFNet consistently surpasses the strong baseline by \${\textbackslash}sim\$2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN achieves a single-model single-scale AP of 55.1 on COCO test-dev, which is state-of-the-art among various object detectors.Code is available at https://github.com/hyz-xmaster/VarifocalNet .},
	language = {en},
	urldate = {2021-06-29},
	author = {Zhang, Haoyang and Wang, Ying and Dayoub, Feras and Sünderhauf, Niko},
	month = aug,
	year = {2020},
	file = {Snapshot:/home/paolo/zotero_data/storage/KCIPY957/2008.html:text/html;Zhang_et_al-2020-VarifocalNet.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2020/Zhang_et_al-2020-VarifocalNet.pdf:application/pdf},
}

@article{brownLanguageModelsAre2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {https://arxiv.org/abs/2005.14165v4},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	language = {en},
	urldate = {2021-06-29},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = may,
	year = {2020},
	file = {Snapshot:/home/paolo/zotero_data/storage/J3QGLXG9/2005.html:text/html;Brown_et_al-2020-Language_Models_are_Few-Shot_Learners.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2020/Brown_et_al-2020-Language_Models_are_Few-Shot_Learners.pdf:application/pdf},
}

@book{sunConnectionistSymbolicIntegrationUnified2013a,
	title = {Connectionist-{Symbolic} {Integration}: {From} {Unified} to {Hybrid} {Approaches}},
	isbn = {978-1-134-80206-7},
	shorttitle = {Connectionist-{Symbolic} {Integration}},
	abstract = {A variety of ideas, approaches, and techniques exist -- in terms of both architecture and learning -- and this abundance seems to lead to many exciting possibilities in terms of theoretical advances and application potentials. Despite the apparent diversity, there is clearly an underlying unifying theme: architectures that bring together symbolic and connectionist models to achieve a synthesis and synergy of the two different paradigms, and the learning and knowledge acquisition methods for developing such architectures. More effort needs to be extended to exploit the possibilities and opportunities in this area.   This book is the outgrowth of The IJCAI Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid Approaches, held in conjunction with the fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95). Featuring various presentations and discussions, this two-day workshop brought to light many new ideas, controversies, and syntheses which lead to the present volume.   This book is concerned with the development, analysis, and application of hybrid connectionist-symbolic models in artificial intelligence and cognitive science. Drawing contributions from a large international group of experts, it describes and compares a variety of models in this area. The types of models discussed cover a wide range of the evolving spectrum of hybrid models, thus serving as a well-balanced progress report on the state of the art. As such, this volume provides an information clearinghouse for various proposed approaches and models that share the common belief that connectionist and symbolic models can be usefully combined and integrated, and such integration may lead to significant advances in understanding intelligence.},
	language = {en},
	publisher = {Psychology Press},
	author = {Sun, Ron and Alexandre, Frederic},
	month = apr,
	year = {2013},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / General},
}

@book{alexandreConnectionistSymbolicIntegrationUnified1997,
	title = {Connectionist-{Symbolic} {Integration}: {From} {Unified} to {Hybrid} {Approaches}},
	shorttitle = {Connectionist-{Symbolic} {Integration}},
	publisher = {Lawrence Erlbaum Associates},
	author = {Alexandre, Frederic and Sun, Ron},
	year = {1997},
}

@article{yeSurveyCognitiveArchitectures2018,
	title = {A {Survey} of {Cognitive} {Architectures} in the {Past} 20 {Years}},
	volume = {48},
	issn = {2168-2267, 2168-2275},
	url = {https://ieeexplore.ieee.org/document/8424435/},
	doi = {10.1109/TCYB.2018.2857704},
	number = {12},
	urldate = {2021-06-29},
	journal = {IEEE Transactions on Cybernetics},
	author = {Ye, Peijun and Wang, Tao and Wang, Fei-Yue},
	month = dec,
	year = {2018},
	pages = {3280--3290},
}

@article{michelInformalInternetSurvey2018,
	title = {An {Informal} {Internet} {Survey} on the {Current} {State} of {Consciousness} {Science}},
	volume = {9},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02134/full},
	doi = {10.3389/fpsyg.2018.02134},
	abstract = {The scientific study of consciousness emerged as an organized field of research only a few decades ago. As the solid empirical findings have greatly enhanced our understanding of consciousness, it is important to find out whether other factors, such as funding for consciousness research and status of consciousness scientists, provide a suitable environment for the field to grow and develop sustainably. We conducted an online survey from January to February in 2018 on people’s view regarding various aspects in the scientific study of consciousness as a field of research. 249 participants completed the survey, among which 80\% were in academia, and around 40\% were experts in consciousness research. Topics covered include the progress made by the field, funding for consciousness research, job opportunities for consciousness researchers, and the scientific rigor of the work done by researchers in the field. The majority of respondents (78\%) indicated that scientific research in consciousness had been making progress. However, among those who knew about the situation (around 55\% of all respondents), most (around 70-80\%) perceived obtaining funding and getting a job in the field of consciousness research as more difficult than in other subfields of neuroscience. Overall, work done in consciousness research was perceived to be less rigorous than other neurosciences subfields (around 80\% out of the 42\% who knew about the situation), but this perceived lack of rigor was not related to the perceived difficulty in finding jobs and obtaining funding. Lastly, we found that, overall, the global workspace theory was perceived to be the most promising (around 28\%), while most non-expert researchers (around 22\% of non-experts) found the information integration theory (IIT) most promising. We believe the survey results provide an interesting picture of current opinions from scientists and researchers about the progresses made and the challenges faced by consciousness research as an independent field. They will inspire collective reflection on the future directions regarding funding and job opportunities for the field.},
	language = {English},
	urldate = {2021-06-29},
	journal = {Frontiers in Psychology},
	author = {Michel, Matthias and Fleming, Stephen M. and Lau, Hakwan and Lee, Alan L. F. and Martinez-Conde, Susana and Passingham, Richard E. and Peters, Megan A. K. and Rahnev, Dobromir and Sergent, Claire and Liu, Kayuet},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {Consciousness, consciousness research, consciousness science, meta-science, Survey},
	file = {Michel_et_al-2018-An_Informal_Internet_Survey_on_the_Current_State_of_Consciousness_Science.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2018/Michel_et_al-2018-An_Informal_Internet_Survey_on_the_Current_State_of_Consciousness_Science.pdf:application/pdf},
}

@article{okeefeDualPhaseRate2005,
	title = {Dual phase and rate coding in hippocampal place cells: {Theoretical} significance and relationship to entorhinal grid cells},
	volume = {15},
	copyright = {Copyright © 2005 Wiley-Liss, Inc.},
	issn = {1098-1063},
	shorttitle = {Dual phase and rate coding in hippocampal place cells},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.20115},
	doi = {10.1002/hipo.20115},
	abstract = {We review the ideas and data behind the hypothesis that hippocampal pyramidal cells encode information by their phase of firing relative to the theta rhythm of the EEG. Particular focus is given to the further hypothesis that variations in firing rate can encode information independently from that encoded by firing phase. We discuss possible explanation of the phase-precession effect in terms of interference between two independent oscillatory influences on the pyramidal cell membrane potential, and the extent to which firing phase reflects internal dynamics or external (environmental) variables. Finally, we propose a model of the firing of the recently discovered “grid cells” in entorhinal cortex as part of a path-integration system, in combination with place cells and head-direction cells. © 2005 Wiley-Liss, Inc.},
	language = {en},
	number = {7},
	urldate = {2021-06-29},
	journal = {Hippocampus},
	author = {O'Keefe, John and Burgess, Neil},
	year = {2005},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.20115},
	keywords = {cognitive map, computational model, hippocampus, oscillation, theta},
	pages = {853--866},
	file = {O'Keefe_Burgess-2005-Dual_phase_and_rate_coding_in_hippocampal_place_cells.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2005/O'Keefe_Burgess-2005-Dual_phase_and_rate_coding_in_hippocampal_place_cells.pdf:application/pdf;Snapshot:/home/paolo/zotero_data/storage/CDW9KHYG/hipo.html:text/html},
}

@article{moserPlaceCellsGrid2008,
	title = {Place {Cells}, {Grid} {Cells}, and the {Brain}'s {Spatial} {Representation} {System}},
	volume = {31},
	issn = {0147-006X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.31.061307.090723},
	doi = {10.1146/annurev.neuro.31.061307.090723},
	abstract = {More than three decades of research have demonstrated a role for hippocampal place cells in representation of the spatial environment in the brain. New studies have shown that place cells are part of a broader circuit for dynamic representation of self-location. A key component of this network is the entorhinal grid cells, which, by virtue of their tessellating firing fields, may provide the elements of a path integration–based neural map. Here we review how place cells and grid cells may form the basis for quantitative spatiotemporal representation of places, routes, and associated experiences during behavior and in memory. Because these cell types have some of the most conspicuous behavioral correlates among neurons in nonsensory cortical systems, and because their spatial firing structure reflects computations internally in the system, studies of entorhinal-hippocampal representations may offer considerable insight into general principles of cortical network dynamics.},
	number = {1},
	urldate = {2021-06-29},
	journal = {Annual Review of Neuroscience},
	author = {Moser, Edvard I. and Kropff, Emilio and Moser, May-Britt},
	month = jul,
	year = {2008},
	note = {Publisher: Annual Reviews},
	pages = {69--89},
	file = {Snapshot:/home/paolo/zotero_data/storage/ZZEJUAEB/annurev.neuro.31.061307.html:text/html},
}

@article{ltdNews2010,
	title = {News},
	volume = {58},
	copyright = {© 2015 American Federation for Medical Research, Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {1081-5589, 1708-8267},
	url = {https://jim.bmj.com/content/58/8/929},
	doi = {10.2310/JIM.0b013e3182025955},
	abstract = {The National Institutes of Health (NIH) awarded \$40 million in grants to support two collaborating research consortia as part of the Human Connectome Project (HCP). The grants are the first awarded under the HCP, which was launched in 2009 by the NIH Blueprint for Neuroscience Research in an effort to utilize the most advanced brain imaging technologies to map the circuitry of the healthy adult human brain. HCP funding comes from 16 components of NIH under the Blueprint for Neuroscience Research. Researchers at Washington University, St. Louis (WUSTL), and the University of Minnesota (UMN) will lead one of the consortia, while the other will be led by investigators at Massachusetts General Hospital (MGH)/Harvard University, Boston and the University of California Los Angeles (UCLA).

The WUSTL/UMN team will work with healthy adult twin pairs and their siblings from 300 families to map each of their connectomes. Resulting maps will demonstrate the anatomical and functional connections between parts of the brain for each individual, and will be related to behavioral test data. Researchers will compare connectomes and genetic data of genetically identical twins with fraternal twins to determine relative genetic and environmental contributions in brain circuitry development and to identify relevant genetic variation. All participants will undergo structural and functional MRI scans while at rest and when challenged by activity. A subset will also be scanned using more powerful 7 and 10.5 Tesla MRI units at the UMN, and yet another subset will include video capture of millisecond brain electrical activity. Magnetoencephalography and electroencephalography will take place at WUSTL. After analysis, the resulting data will become web accessible via a customized Connectome Database Neuroinformatics Platform. Nine research centers, which include national and international institutions, will collaborate on this five-year project.

The MGH/Harvard-UCLA Connectome consortium will be focused on optimizing MRI technology using …},
	language = {en},
	number = {8},
	urldate = {2021-06-29},
	journal = {Journal of Investigative Medicine},
	author = {Ltd, BMJ Publishing Group},
	month = dec,
	year = {2010},
	note = {Publisher: BMJ Publishing Group Limited
Section: News},
	pages = {929--935},
	file = {Snapshot:/home/paolo/zotero_data/storage/A37AEBJE/929.html:text/html},
}

@misc{NIHBRAINInitiative2013,
	title = {The {NIH} {BRAIN} {Initiative} {\textbar} {Science}},
	url = {https://science.sciencemag.org/content/340/6133/687},
	urldate = {2021-06-29},
	year = {2013},
	file = {The NIH BRAIN Initiative | Science:/home/paolo/zotero_data/storage/7DV7WSH5/687.html:text/html},
}

@misc{CountdownDigitalSimulation2012,
	title = {A {Countdown} to a {Digital} {Simulation} of {Every} {Last} {Neuron} in the {Human} {Brain}},
	url = {https://www.scientificamerican.com/article/human-brain-project-digital-simulation-neuron/},
	abstract = {Building a vast digital simulation of the brain could transform neuroscience and medicine and reveal new ways of making more powerful computers},
	language = {en},
	urldate = {2021-06-29},
	journal = {Scientific American},
	month = jun,
	year = {2012},
	doi = {10.1038/scientificamerican0612-50},
	file = {Snapshot:/home/paolo/zotero_data/storage/8G6J6IWL/human-brain-project-digital-simulation-neuron.html:text/html},
}

@article{yeungChangingLandscapeNeuroscience2017,
	title = {The {Changing} {Landscape} of {Neuroscience} {Research}, 2006–2015: {A} {Bibliometric} {Study}},
	volume = {11},
	issn = {1662-453X},
	shorttitle = {The {Changing} {Landscape} of {Neuroscience} {Research}, 2006–2015},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00120/full#B26},
	doi = {10.3389/fnins.2017.00120},
	abstract = {Background: It is beneficial to evaluate changes in neuroscience research field regarding research directions and topics over a defined period. Such information enables stakeholders to quickly identify the most influential research and incorporate latest evidence into research-informed education. To our knowledge, no study reported changes in neuroscience literature over the last decade. Therefore, the current study determined research terms with highest citation scores, compared publication shares of research areas and contributing countries in this field from 2006–2015 and identified the most productive journals. Methods: Data were extracted from Web of Science and Journal Citation Reports (JCR). Only articles and reviews published in journals classified under the JCR “Neurosciences” category over the period of interest were included. Title and abstract fields of each included publication were extracted and analyzed via VOSviewer to identify recurring terms with high relative citation scores. Two term maps were produced for publications over the study period to illustrate the extent of co-occurrence, and the impact of terms was evaluated based on their relative citation scores. To further describe the recent research priority or “hot spots,” ten terms with the highest relative citation scores were identified annually. In addition, by applying Bradford’s law, we identified ten journals being the most productive journals per annum over the survey period and evaluated their bilbiometric performances. Results: From 2006–2015, there were 47 terms involved in the annual lists of top ten terms with highest relative citation scores. The most frequently recurring terms were autism (8), meta-analysis (7), functional connectivity (6), default mode network (4) and neuroimaging (4). Neuroscience research related to psychology and behavioral sciences showed an increase in publication share over the survey period, and China has become one of the major contributors to neuroscience research. Ten journals were frequently identified (≥8 years) as core journals within the survey period. Discussion: The landscape of neuroscience research has changed recently, and this paper provides contemporary overview for researchers and health care workers interested in this field’s research and developments. Brain imaging and brain connectivity terms had high relative citation scores.},
	language = {English},
	urldate = {2021-06-29},
	journal = {Frontiers in Neuroscience},
	author = {Yeung, Andy Wai Kan and Goto, Tazuko K. and Leung, W. Keung},
	year = {2017},
	note = {Publisher: Frontiers},
	keywords = {Bibliometrics, Cells, Diagnostic Imaging, Functional Neuroimaging, Information Science, Literature Based Discovery, Neurosciences},
	file = {Yeung_et_al-2017-The_Changing_Landscape_of_Neuroscience_Research,_2006–2015.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2017/Yeung_et_al-2017-The_Changing_Landscape_of_Neuroscience_Research,_2006–2015.pdf:application/pdf},
}

@misc{MYCINArtificialIntelligence,
	title = {{MYCIN} {\textbar} artificial intelligence program},
	url = {https://www.britannica.com/technology/MYCIN},
	abstract = {MYCIN, an early expert system, or artificial intelligence (AI) program, for treating blood infections. In 1972 work began on MYCIN at Stanford University in California. MYCIN would attempt to diagnose patients based on reported symptoms and medical test results. The program could request further},
	language = {en},
	urldate = {2021-06-29},
	journal = {Encyclopedia Britannica},
	file = {Snapshot:/home/paolo/zotero_data/storage/YTB3I2KB/MYCIN.html:text/html},
}

@article{garcezNeurosymbolicAI3rd2020a,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {Neurosymbolic {AI}},
	url = {http://arxiv.org/abs/2012.05876},
	abstract = {Current advances in Artiﬁcial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by inﬂuential thinkers. Many have identiﬁed the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	language = {en},
	urldate = {2021-06-29},
	journal = {arXiv:2012.05876 [cs]},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.05876},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	file = {Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf:/home/paolo/zotero_data/storage/RWATELLH/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf:application/pdf},
}

@article{smolenskyConnectionistAISymbolic1987,
	title = {Connectionist {AI}, symbolic {AI}, and the brain},
	volume = {1},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/BF00130011},
	doi = {10.1007/BF00130011},
	language = {en},
	number = {2},
	urldate = {2021-07-01},
	journal = {Artificial Intelligence Review},
	author = {Smolensky, P.},
	year = {1987},
	pages = {95--109},
	file = {Smolensky-1987-Connectionist_AI,_symbolic_AI,_and_the_brain.pdf:/home/paolo/Documents/scuola/academic_p/sorted/1987/Smolensky-1987-Connectionist_AI,_symbolic_AI,_and_the_brain.pdf:application/pdf},
}

@article{mozerUsingRelevanceReduce1989,
	title = {Using {Relevance} to {Reduce} {Network} {Size} {Automatically}},
	volume = {1},
	issn = {0954-0091, 1360-0494},
	url = {https://www.tandfonline.com/doi/full/10.1080/09540098908915626},
	doi = {10.1080/09540098908915626},
	language = {en},
	number = {1},
	urldate = {2021-07-01},
	journal = {Connection Science},
	author = {Mozer, Michael C. and Smolensky, Paul},
	month = jan,
	year = {1989},
	pages = {3--16},
	file = {Mozer and Smolensky - 1989 - Using Relevance to Reduce Network Size Automatical.pdf:/home/paolo/zotero_data/storage/GGVAZISL/Mozer and Smolensky - 1989 - Using Relevance to Reduce Network Size Automatical.pdf:application/pdf},
}

@misc{kautzAAAI2020TalkSlides2020,
	title = {{AAAI2020} {Talk} {Slides}},
	url = {https://www.cs.rochester.edu/u/kautz/talks/Kautz%20Engelmore%20Lecture%20Directors%20Cut.pdf},
	urldate = {2021-07-01},
	author = {Kautz, Henry},
	month = feb,
	year = {2020},
	file = {Kautz Engelmore Lecture Directors Cut.pdf:/home/paolo/zotero_data/storage/NCLLHY98/Kautz Engelmore Lecture Directors Cut.pdf:application/pdf},
}

@inproceedings{yiNeuralsymbolicVQADisentangling2018,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {Neural-symbolic {VQA}: disentangling reasoning from vision and language understanding},
	shorttitle = {Neural-symbolic {VQA}},
	abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
	urldate = {2021-07-01},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Joshua B.},
	month = dec,
	year = {2018},
	pages = {1039--1050},
	file = {Yi_et_al-2018-Neural-symbolic_VQA.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2018/Yi_et_al-2018-Neural-symbolic_VQA.pdf:application/pdf},
}

@article{lampleDeepLearningSymbolic2019,
	title = {Deep {Learning} for {Symbolic} {Mathematics}},
	url = {http://arxiv.org/abs/1912.01412},
	abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
	urldate = {2021-07-01},
	journal = {arXiv:1912.01412 [cs]},
	author = {Lample, Guillaume and Charton, François},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.01412},
	keywords = {Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {Lample_Charton-2019-Deep_Learning_for_Symbolic_Mathematics.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2019/Lample_Charton-2019-Deep_Learning_for_Symbolic_Mathematics.pdf:application/pdf;arXiv.org Snapshot:/home/paolo/zotero_data/storage/KWHD26H9/1912.html:text/html},
}

@article{mccoyRNNsImplicitlyImplement2019,
	title = {{RNNs} {Implicitly} {Implement} {Tensor} {Product} {Representations}},
	url = {http://arxiv.org/abs/1812.08718},
	abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations.},
	urldate = {2021-07-01},
	journal = {arXiv:1812.08718 [cs]},
	author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.08718},
	keywords = {Computer Science - Computation and Language},
	file = {McCoy_et_al-2019-RNNs_Implicitly_Implement_Tensor_Product_Representations.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2019/McCoy_et_al-2019-RNNs_Implicitly_Implement_Tensor_Product_Representations.pdf:application/pdf;arXiv.org Snapshot:/home/paolo/zotero_data/storage/M6QSFEQQ/1812.html:text/html},
}

@article{serafiniLogicTensorNetworks2016,
	title = {Logic {Tensor} {Networks}: {Deep} {Learning} and {Logical} {Reasoning} from {Data} and {Knowledge}},
	shorttitle = {Logic {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1606.04422},
	abstract = {We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.},
	urldate = {2021-07-01},
	journal = {arXiv:1606.04422 [cs]},
	author = {Serafini, Luciano and Garcez, Artur d'Avila},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.04422},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Serafini_Garcez-2016-Logic_Tensor_Networks.pdf:/home/paolo/Documents/scuola/academic_p/sorted/2016/Serafini_Garcez-2016-Logic_Tensor_Networks.pdf:application/pdf;arXiv.org Snapshot:/home/paolo/zotero_data/storage/P7ZD7DG6/1606.html:text/html},
}
